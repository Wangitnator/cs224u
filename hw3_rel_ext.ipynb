{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney\"\n",
    "__version__ = \"CS224U, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Baseline](#Baseline)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Different model factory [1 point]](#Different-model-factory-[1-point])\n",
    "  1. [Directional unigram features [2 points]](#Directional-unigram-features-[2-points])\n",
    "  1. [The part-of-speech tags of the \"middle\" words [2 points]](#The-part-of-speech-tags-of-the-\"middle\"-words-[2-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to the developing really effective relation extraction systems using distant supervision. \n",
    "\n",
    "As with the previous assignments, this notebook first establishes a baseline system. The initial homework questions ask you to create additional baselines and suggest areas for innovation, and the final homework question asks you to develop an original system for you to enter into the bake-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geograpy3\n",
      "  Using cached https://files.pythonhosted.org/packages/57/3e/881a580f03ee257b61b157bc82b011d35cb3174da1b5f48dbeae38c64b87/geograpy3-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /data/anaconda/envs/py35/lib/python3.5/site-packages (from geograpy3) (1.15.0)\n",
      "Requirement already satisfied: jellyfish in /data/anaconda/envs/py35/lib/python3.5/site-packages (from geograpy3) (0.7.2)\n",
      "Requirement already satisfied: nltk in /data/anaconda/envs/py35/lib/python3.5/site-packages (from geograpy3) (3.3)\n",
      "Requirement already satisfied: pycountry in /data/anaconda/envs/py35/lib/python3.5/site-packages (from geograpy3) (19.8.18)\n",
      "Requirement already satisfied: newspaper3k in /data/anaconda/envs/py35/lib/python3.5/site-packages (from geograpy3) (0.2.8)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py35/lib/python3.5/site-packages (from nltk->geograpy3) (1.11.0)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (2.2.2)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (3.13)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (0.3)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (2.8.1)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (4.2.5)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (0.0.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (4.6.3)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (0.35.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (5.2.1)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from newspaper3k->geograpy3) (5.1.0)\n",
      "Requirement already satisfied: setuptools in /data/anaconda/envs/py35/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k->geograpy3) (40.2.0)\n",
      "Requirement already satisfied: idna in /data/anaconda/envs/py35/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k->geograpy3) (2.7)\n",
      "Requirement already satisfied: requests-file>=1.4 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from tldextract>=2.0.1->newspaper3k->geograpy3) (1.4.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k->geograpy3) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k->geograpy3) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.10.0->newspaper3k->geograpy3) (2018.8.24)\n",
      "Installing collected packages: geograpy3\n",
      "Successfully installed geograpy3-0.1.2\n",
      "\u001b[33mWARNING: You are using pip version 19.2.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install geograpy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import rel_ext\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE\n",
      "<class 'nltk.tree.Tree'>\n",
      "obt\n"
     ]
    }
   ],
   "source": [
    "sbt_obj = \"Stockholm\"\n",
    "obt_obj = \"Bergen\"\n",
    "#feature_counter[\"sbt_obj\"] = 0\n",
    "#feature_counter[\"obt_obj\"] = 0\n",
    "text = nltk.word_tokenize(sbt_obj)\n",
    "nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "for ne in nes:\n",
    "    if type(ne) is nltk.tree.Tree and ne.label() == \"GPE\":\n",
    "        print(ne.label())\n",
    "        #feature_counter[\"sbt_obj\"] = 1\n",
    "\n",
    "text = nltk.word_tokenize(obt_obj)\n",
    "nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "for ne in nes:\n",
    "    if type(ne) is nltk.tree.Tree and ne.label() == \"GPE\":\n",
    "        print(\"obt\")\n",
    "        #feature_counter[\"obt_obj\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we unite our corpus and KB into a dataset, and create some splits for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_HOME = '/home/kd/data/data'\n",
    "rel_ext_data_home = os.path.join(DATA_HOME, 'rel_ext_data')\n",
    "GLOVE_HOME = os.path.join(DATA_HOME, 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not wedded to this set-up for splits. The bake-off will be conducted on a previously unseen test-set, so all of the data in `dataset` is fair game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.79, 0.20],\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': Corpus with 331,696 examples; KB with 45,884 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 263,285 examples; KB with 36,191 triples}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter, \n",
    "                                    use_middle_length=False, \n",
    "                                    use_entities=False,\n",
    "                                    context_section='middle', # can be 'left', 'right', or 'middle'\n",
    "                                    use_synsets=False):\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "        \n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word, pos_word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[syn.lemma()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        \n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "\n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word, pos_word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[syn.lemma()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "featurizers = [simple_bag_of_words_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')\n",
    "model_factory_2k = lambda: LogisticRegression(fit_intercept=True, solver='liblinear', max_iter=2000)\n",
    "model_factory_4k = lambda: LogisticRegression(fit_intercept=True, solver='liblinear', max_iter=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.864      0.374      0.684        340       5716\n",
      "parents                   0.834      0.532      0.749        312       5688\n",
      "founders                  0.835      0.413      0.693        380       5756\n",
      "genre                     0.558      0.171      0.384        170       5546\n",
      "worked_at                 0.739      0.269      0.547        242       5618\n",
      "contains                  0.797      0.605      0.750       3904       9280\n",
      "author                    0.830      0.507      0.736        509       5885\n",
      "nationality               0.675      0.179      0.435        301       5677\n",
      "film_performance          0.783      0.547      0.721        766       6142\n",
      "place_of_birth            0.657      0.197      0.448        233       5609\n",
      "is_a                      0.671      0.225      0.481        497       5873\n",
      "place_of_death            0.500      0.126      0.313        159       5535\n",
      "has_sibling               0.860      0.246      0.574        499       5875\n",
      "capital                   0.595      0.232      0.453         95       5471\n",
      "profession                0.647      0.178      0.424        247       5623\n",
      "has_spouse                0.835      0.333      0.642        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.730      0.321      0.565       9248      95264\n"
     ]
    }
   ],
   "source": [
    "baseline_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying model weights might yield insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     2.542 Taluks\n",
      "     2.473 Córdoba\n",
      "     2.453 Valais\n",
      "     ..... .....\n",
      "    -1.336 America\n",
      "    -1.382 Spain\n",
      "    -2.202 Earth\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     5.260 son\n",
      "     4.418 daughter\n",
      "     4.296 father\n",
      "     ..... .....\n",
      "    -1.355 Tyndareus\n",
      "    -1.597 Tina\n",
      "    -1.843 played\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     4.087 founder\n",
      "     3.728 founded\n",
      "     3.064 co-founder\n",
      "     ..... .....\n",
      "    -1.589 MD\n",
      "    -1.600 novel\n",
      "    -1.926 band\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     2.787 series\n",
      "     2.647 movie\n",
      "     2.456 album\n",
      "     ..... .....\n",
      "    -1.702 starring\n",
      "    -1.783 at\n",
      "    -1.955 original\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     3.077 CEO\n",
      "     2.860 professor\n",
      "     2.672 president\n",
      "     ..... .....\n",
      "    -1.374 India\n",
      "    -1.651 or\n",
      "    -1.746 state\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     2.313 districts\n",
      "     2.174 third-largest\n",
      "     2.146 bordered\n",
      "     ..... .....\n",
      "    -3.074 Lancashire\n",
      "    -3.186 Midlands\n",
      "    -3.551 Ceylon\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     2.405 book\n",
      "     2.311 wrote\n",
      "     2.263 books\n",
      "     ..... .....\n",
      "    -2.133 1925\n",
      "    -2.967 Daisy\n",
      "    -5.863 dystopian\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     2.644 born\n",
      "     1.837 Set\n",
      "     1.823 caliph\n",
      "     ..... .....\n",
      "    -1.529 American\n",
      "    -1.704 U.S.\n",
      "    -1.957 state\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     4.023 co-starring\n",
      "     4.012 starring\n",
      "     3.407 opposite\n",
      "     ..... .....\n",
      "    -2.113 She\n",
      "    -2.228 Anjaani\n",
      "    -2.228 Anjaana\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     3.819 born\n",
      "     2.947 birthplace\n",
      "     2.400 mayor\n",
      "     ..... .....\n",
      "    -1.462 and\n",
      "    -1.815 state\n",
      "    -1.994 Oldham\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     2.787 family\n",
      "     2.646 genus\n",
      "     2.567 \n",
      "     ..... .....\n",
      "    -1.608 on\n",
      "    -1.631 now\n",
      "    -1.704 nightshade\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     2.378 died\n",
      "     1.988 assassinated\n",
      "     1.901 son\n",
      "     ..... .....\n",
      "    -1.130 province\n",
      "    -1.214 and\n",
      "    -1.411 state\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     5.375 brother\n",
      "     4.316 sister\n",
      "     3.003 Marlon\n",
      "     ..... .....\n",
      "    -1.211 starring\n",
      "    -1.229 –\n",
      "    -1.402 from\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     3.536 capital\n",
      "     1.747 city\n",
      "     1.727 posted\n",
      "     ..... .....\n",
      "    -1.196 and\n",
      "    -1.257 borough\n",
      "    -1.408 States\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     3.076 \n",
      "     2.535 vocalist\n",
      "     2.377 philosopher\n",
      "     ..... .....\n",
      "    -1.249 in\n",
      "    -1.256 Texas\n",
      "    -2.147 on\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     5.509 wife\n",
      "     4.420 married\n",
      "     4.367 widow\n",
      "     ..... .....\n",
      "    -1.282 Tyndareus\n",
      "    -1.319 children\n",
      "    -1.660 Terri\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different model factory [1 point]\n",
    "\n",
    "The code in `rel_ext` makes it very easy to experiment with other classifier models: one need only redefine the `model_factory` argument. This question asks you to assess a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "__To submit:__ A call to `rel_ext.experiment` training on the 'train' part of `splits` and assessing on its `dev` part, with `featurizers` as defined above in this notebook and the `model_factory` set to one based in an `SVC` with `kernel='linear'` and all other arguments left with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_factory = lambda: SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.752      0.627      0.723        766       6142\n",
      "place_of_birth            0.602      0.215      0.442        233       5609\n",
      "nationality               0.496      0.189      0.375        301       5677\n",
      "place_of_death            0.450      0.113      0.282        159       5535\n",
      "has_spouse                0.842      0.342      0.651        594       5970\n",
      "genre                     0.516      0.276      0.440        170       5546\n",
      "founders                  0.725      0.424      0.635        380       5756\n",
      "has_sibling               0.774      0.255      0.550        499       5875\n",
      "is_a                      0.608      0.284      0.495        497       5873\n",
      "profession                0.577      0.259      0.463        247       5623\n",
      "worked_at                 0.622      0.306      0.515        242       5618\n",
      "contains                  0.777      0.602      0.735       3904       9280\n",
      "author                    0.717      0.611      0.693        509       5885\n",
      "capital                   0.765      0.274      0.563         95       5471\n",
      "adjoins                   0.822      0.312      0.619        340       5716\n",
      "parents                   0.783      0.590      0.735        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.677      0.355      0.557       9248      95264\n"
     ]
    }
   ],
   "source": [
    "svc_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=svc_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional unigram features [2 points]\n",
    "\n",
    "The current bag-of-words representation makes no distinction between \"forward\" and \"reverse\" examples. But, intuitively, there is big difference between _X and his son Y_ and _Y and his son X_. This question asks you to modify `simple_bag_of_words_featurizer` to capture these differences. \n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `directional_bag_of_words_featurizer` that is just like `simple_bag_of_words_featurizer` except that it distinguishes \"forward\" and \"reverse\". To do this, you just need to mark each word feature for whether it is derived from a subject–object example or from an object–subject example. The precise nature of the mark you add for the two cases doesn't make a difference to the model.\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `directional_bag_of_words_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "3. `rel_ext.experiment` returns some of the core objects used in the experiment. How many feature names does the `vectorizer` have for the experiment run in the previous step? (Note: we're partly asking you to figure out how to get this value by using the sklearn documentation, so please don't ask how to do it on Piazza!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter, fwd_prefix='$FWD_DIRECTION: ', \n",
    "                                        bwd_prefix='$BWD_DIREECTION: ', use_middle_length=False,\n",
    "                                        use_entities=False, include_left=False, include_right=False):\n",
    "    \"\"\"feature_counter[\"sbt.obj\"] = 0\n",
    "    feature_counter[\"obt.obj\"] = 0\n",
    "    \n",
    "    text = nltk.word_tokenize(kbt.sbj)\n",
    "    nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "    for ne in nes:\n",
    "        if type(ne) is nltk.tree.Tree and ne.label() == \"GPE\":\n",
    "            feature_counter[\"sbt.obj\"] = 1\n",
    "\n",
    "    text = nltk.word_tokenize(kbt.obj)\n",
    "    nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "    for ne in nes:\n",
    "        if type(ne) is nltk.tree.Tree and ne.label() == \"GPE\":\n",
    "            feature_counter[\"obt.obj\"] = 1\"\"\"\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            word_direction = fwd_prefix + \"_middle_\" + word\n",
    "            feature_counter[word_direction] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter[fwd_prefix + 'NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if include_left:\n",
    "            words = ex.left.split(' ')\n",
    "            for word in words:\n",
    "                word_direction = fwd_prefix + \"_left_\" + word\n",
    "                feature_counter[word_direction] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter[fwd_prefix + 'NUM_WORD_IN_LEFT']  += len(words)\n",
    "        if include_right:\n",
    "            words = ex.right.split(' ')\n",
    "            for word in words:\n",
    "                word_direction = fwd_prefix + \"_right_\" + word\n",
    "                feature_counter[word_direction] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter[fwd_prefix + 'NUM_WORD_IN_RIGHT']  += len(words)                \n",
    "        if use_entities:\n",
    "            feature_counter[fwd_prefix + kbt.sbj] += 1\n",
    "            feature_counter[bwd_prefix + kbt.obj] += 1\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            word_direction = bwd_prefix +\"_middle_\" + word\n",
    "            feature_counter[word_direction] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['BWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "            \n",
    "        if include_left:\n",
    "            words = ex.left.split(' ')\n",
    "            for word in words:\n",
    "                word_direction = bwd_prefix + \"_left_\" + word\n",
    "                feature_counter[word_direction] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter['BWD_NUM_WORD_IN_LEFT']  += len(words)\n",
    "        if include_right:\n",
    "            words = ex.right.split(' ')\n",
    "            for word in words:\n",
    "                word_direction = bwd_prefix + \"_right_\" + word\n",
    "                feature_counter[word_direction] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter[bwd_prefix +\"BWD_NUM_WORD_IN_RIGHT'\"]  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[fwd_prefix + kbt.sbj] += 1\n",
    "            feature_counter[bwd_prefix + kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "genre                     0.743      0.306      0.578        170       5546\n",
      "worked_at                 0.717      0.273      0.541        242       5618\n",
      "has_spouse                0.844      0.347      0.656        594       5970\n",
      "profession                0.707      0.235      0.504        247       5623\n",
      "author                    0.862      0.615      0.798        509       5885\n",
      "has_sibling               0.866      0.246      0.576        499       5875\n",
      "adjoins                   0.849      0.415      0.702        340       5716\n",
      "contains                  0.800      0.737      0.786       3904       9280\n",
      "film_performance          0.841      0.636      0.790        766       6142\n",
      "capital                   0.647      0.232      0.476         95       5471\n",
      "parents                   0.856      0.516      0.757        312       5688\n",
      "place_of_death            0.600      0.132      0.351        159       5535\n",
      "nationality               0.667      0.239      0.491        301       5677\n",
      "founders                  0.795      0.397      0.662        380       5756\n",
      "is_a                      0.699      0.243      0.509        497       5873\n",
      "place_of_birth            0.671      0.245      0.497        233       5609\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.760      0.363      0.605       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.849      0.654      0.801        301       5677\n",
      "adjoins                   0.752      0.285      0.567        340       5716\n",
      "place_of_death            0.821      0.403      0.679        159       5535\n",
      "parents                   0.880      0.657      0.824        312       5688\n",
      "contains                  0.862      0.624      0.801       3904       9280\n",
      "worked_at                 0.757      0.231      0.520        242       5618\n",
      "genre                     0.804      0.265      0.571        170       5546\n",
      "has_spouse                0.834      0.584      0.768        594       5970\n",
      "profession                0.890      0.490      0.765        247       5623\n",
      "place_of_birth            0.791      0.438      0.681        233       5609\n",
      "founders                  0.727      0.274      0.546        380       5756\n",
      "has_sibling               0.858      0.627      0.799        499       5875\n",
      "author                    0.845      0.717      0.816        509       5885\n",
      "film_performance          0.854      0.678      0.811        766       6142\n",
      "capital                   0.480      0.126      0.308         95       5471\n",
      "is_a                      0.876      0.541      0.780        497       5873\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.805      0.475      0.690       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_bag_left_right_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(directional_bag_of_words_featurizer, use_middle_length=True, \n",
    "                         use_entities=True, include_left=True, include_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The part-of-speech tags of the \"middle\" words [2 points]\n",
    "\n",
    "Our corpus distribution contains part-of-speech (POS) tagged versions of the core text spans. Let's begin to explore whether there is information in these sequences, focusing on `middle_POS`.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `middle_bigram_pos_tag_featurizer` that is just like `simple_bag_of_words_featurizer` except that it creates a feature for bigram POS sequences. For example, given \n",
    "\n",
    "  `The/DT dog/N napped/V`\n",
    "  \n",
    "   we obtain the list of bigram POS sequences\n",
    "  \n",
    "   `b = ['<s> DT', 'DT N', 'N V', 'V </s>']`. \n",
    "   \n",
    "   Of course, `middle_bigram_pos_tag_featurizer` should return count dictionaries defined in terms of such bigram POS lists, on the model of `simple_bag_of_words_featurizer`.\n",
    "   \n",
    "   Don't forget the start and end tags, to model those environments properly!\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `middle_bigram_pos_tag_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "Note: To parse `middle_POS`, one splits on whitespace to get the `word/TAG` pairs. Each of these pairs `s` can be parsed with `s.rsplit('/', 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_featurize(pos_segments, feature_counter, prefix=\"\"):\n",
    "    word_POSs = pos_segments.split(' ')\n",
    "    len_POS = len(word_POSs)\n",
    "    for i in range(-1, len_POS - 1):\n",
    "        pos = word_POSs[i].rsplit('/', 1)\n",
    "        bigram = \"\"\n",
    "        if len(pos) > 1:\n",
    "            if i == -1:\n",
    "                bigram = '<s> ' + pos[1]\n",
    "            elif i == len_POS - 2:\n",
    "                bigram = pos[1] + ' </s>'\n",
    "            else:\n",
    "                bigram = pos[1] + \" \" + word_POSs[i+1].rsplit('/', 1)[1]\n",
    "        feature_counter[prefix + bigram] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=False, use_right=False):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        feature_counter = pos_featurize(ex.middle_POS, feature_counter, \"middle\")\n",
    "        if use_left:\n",
    "            feature_counter = pos_featurize(ex.left_POS, feature_counter, \"left\")\n",
    "        if use_right:\n",
    "            feature_counter = pos_featurize(ex.right_POS, feature_counter, \"right\")\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter)\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.653      0.163      0.408        301       5677\n",
      "author                    0.845      0.246      0.568        509       5885\n",
      "worked_at                 0.607      0.140      0.365        242       5618\n",
      "parents                   0.735      0.240      0.521        312       5688\n",
      "has_spouse                0.777      0.258      0.554        594       5970\n",
      "place_of_death            0.700      0.132      0.376        159       5535\n",
      "profession                0.742      0.186      0.465        247       5623\n",
      "genre                     0.824      0.082      0.294        170       5546\n",
      "capital                   0.500      0.095      0.269         95       5471\n",
      "place_of_birth            0.750      0.206      0.491        233       5609\n",
      "contains                  0.725      0.297      0.563       3904       9280\n",
      "film_performance          0.730      0.261      0.537        766       6142\n",
      "adjoins                   0.878      0.382      0.697        340       5716\n",
      "is_a                      0.672      0.161      0.411        497       5873\n",
      "has_sibling               0.743      0.168      0.442        499       5875\n",
      "founders                  0.697      0.139      0.387        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.724      0.197      0.459       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[middle_bigram_pos_tag_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.552      0.246      0.442        301       5677\n",
      "author                    0.717      0.379      0.609        509       5885\n",
      "worked_at                 0.481      0.260      0.411        242       5618\n",
      "parents                   0.667      0.269      0.515        312       5688\n",
      "has_spouse                0.634      0.311      0.525        594       5970\n",
      "place_of_death            0.471      0.201      0.371        159       5535\n",
      "profession                0.632      0.271      0.499        247       5623\n",
      "genre                     0.588      0.118      0.327        170       5546\n",
      "capital                   0.478      0.116      0.294         95       5471\n",
      "place_of_birth            0.612      0.258      0.480        233       5609\n",
      "contains                  0.743      0.298      0.572       3904       9280\n",
      "film_performance          0.662      0.317      0.544        766       6142\n",
      "adjoins                   0.749      0.465      0.667        340       5716\n",
      "is_a                      0.557      0.225      0.430        497       5873\n",
      "has_sibling               0.662      0.315      0.543        499       5875\n",
      "founders                  0.540      0.197      0.401        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.609      0.265      0.477       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=True, use_right=True)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.594      0.252      0.467        301       5677\n",
      "author                    0.720      0.354      0.596        509       5885\n",
      "worked_at                 0.585      0.256      0.465        242       5618\n",
      "parents                   0.682      0.282      0.531        312       5688\n",
      "has_spouse                0.688      0.301      0.548        594       5970\n",
      "place_of_death            0.536      0.233      0.425        159       5535\n",
      "profession                0.635      0.219      0.460        247       5623\n",
      "genre                     0.621      0.106      0.315        170       5546\n",
      "capital                   0.600      0.126      0.343         95       5471\n",
      "place_of_birth            0.574      0.232      0.443        233       5609\n",
      "contains                  0.740      0.302      0.574       3904       9280\n",
      "film_performance          0.694      0.305      0.553        766       6142\n",
      "adjoins                   0.775      0.456      0.680        340       5716\n",
      "is_a                      0.646      0.209      0.456        497       5873\n",
      "has_sibling               0.705      0.273      0.535        499       5875\n",
      "founders                  0.600      0.197      0.426        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.650      0.256      0.489       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=True, use_right=False)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.634      0.196      0.438        301       5677\n",
      "author                    0.781      0.358      0.632        509       5885\n",
      "worked_at                 0.526      0.211      0.405        242       5618\n",
      "parents                   0.653      0.260      0.501        312       5688\n",
      "has_spouse                0.713      0.293      0.554        594       5970\n",
      "place_of_death            0.525      0.132      0.329        159       5535\n",
      "profession                0.759      0.243      0.533        247       5623\n",
      "genre                     0.579      0.129      0.342        170       5546\n",
      "capital                   0.458      0.116      0.288         95       5471\n",
      "place_of_birth            0.683      0.240      0.499        233       5609\n",
      "contains                  0.720      0.285      0.552       3904       9280\n",
      "film_performance          0.683      0.304      0.547        766       6142\n",
      "adjoins                   0.806      0.441      0.692        340       5716\n",
      "is_a                      0.615      0.209      0.443        497       5873\n",
      "has_sibling               0.680      0.230      0.489        499       5875\n",
      "founders                  0.485      0.132      0.316        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.644      0.236      0.472       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=False, use_right=True)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "There are many options, and this could easily grow into a project. Here are a few ideas:\n",
    "\n",
    "- Try out different classifier models, from `sklearn` and elsewhere.\n",
    "- Add a feature that indicates the length of the middle.\n",
    "- Augment the bag-of-words representation to include bigrams or trigrams (not just unigrams).\n",
    "- Introduce features based on the entity mentions themselves. <!-- \\[SPOILER: it helps a lot, maybe 4% in F-score. And combines nicely with the directional features.\\] -->\n",
    "- Experiment with features based on the context outside (rather than between) the two entity mentions — that is, the words before the first mention, or after the second.\n",
    "- Try adding features which capture syntactic information, such as the dependency-path features used by Mintz et al. 2009. The [NLTK](https://www.nltk.org/) toolkit contains a variety of [parsing algorithms](http://www.nltk.org/api/nltk.parse.html) that may help.\n",
    "- The bag-of-words representation does not permit generalization across word categories such as names of people, places, or companies. Can we do better using word embeddings such as [GloVe](https://nlp.stanford.edu/projects/glove/)?\n",
    "- Consider adding features based on WordNet synsets. Here's a little code to get you started with that:\n",
    "  ```\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  dog_compatible_synsets = wn.synsets('dog', pos='n')\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "simple_bag_of_words_middle_featurizer = partial(simple_bag_of_words_featurizer,use_middle_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.781      0.551      0.721        766       6142\n",
      "place_of_birth            0.623      0.206      0.444        233       5609\n",
      "nationality               0.594      0.199      0.426        301       5677\n",
      "place_of_death            0.526      0.126      0.322        159       5535\n",
      "has_spouse                0.880      0.320      0.652        594       5970\n",
      "genre                     0.604      0.188      0.419        170       5546\n",
      "founders                  0.788      0.392      0.656        380       5756\n",
      "has_sibling               0.861      0.236      0.564        499       5875\n",
      "is_a                      0.707      0.199      0.468        497       5873\n",
      "profession                0.578      0.194      0.415        247       5623\n",
      "worked_at                 0.706      0.248      0.515        242       5618\n",
      "contains                  0.798      0.602      0.749       3904       9280\n",
      "author                    0.787      0.528      0.717        509       5885\n",
      "capital                   0.643      0.189      0.435         95       5471\n",
      "adjoins                   0.886      0.365      0.689        340       5716\n",
      "parents                   0.842      0.545      0.759        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.725      0.318      0.559       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bow_middle_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_middle_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_featurize(words, feature_counter, n, prefix=\"\", directional_prefix=\"\", use_middle_length=False):\n",
    "    for i in range(0, len(words), n):\n",
    "            end = i + n\n",
    "            if (len(words) - i) < n:\n",
    "                end = len(words)\n",
    "            n_gram = ' '.join(words[i:end])\n",
    "            n_gram = directional_prefix + n_gram\n",
    "            feature_counter[prefix + n_gram] += 1\n",
    "    if use_middle_length:\n",
    "        feature_counter[directional_prefix+'NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, \n",
    "                                   directional=False, use_middle_length=False,\n",
    "                                   use_left=False, use_right=False):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        directional_prefix=\"\"\n",
    "        if directional:\n",
    "            directional_prefix = \"FWD_\"\n",
    "        feature_counter = bow_featurize(words, feature_counter, n, \"middle_\", directional_prefix, use_middle_length)\n",
    "        if use_left:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"left_\", directional_prefix, use_middle_length)\n",
    "        if use_right:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"right_\", directional_prefix, use_middle_length)\n",
    "        \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        directional_prefix=\"\"\n",
    "        if directional:\n",
    "            directional_prefix = \"BWD_\"\n",
    "        feature_counter = bow_featurize(words, feature_counter, n, \"middle_\", directional_prefix, use_middle_length)\n",
    "        if use_left:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"left_\", directional_prefix, use_middle_length)\n",
    "        if use_right:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"right_\", directional_prefix, use_middle_length)\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_bag_of_words_featurizer = partial(ngrams_bag_of_words_featurizer, n=2)\n",
    "trigrams_bag_of_words_featurizer = partial(ngrams_bag_of_words_featurizer, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.677      0.146      0.392        301       5677\n",
      "author                    0.849      0.409      0.698        509       5885\n",
      "worked_at                 0.700      0.174      0.436        242       5618\n",
      "parents                   0.910      0.388      0.717        312       5688\n",
      "has_spouse                0.915      0.273      0.622        594       5970\n",
      "place_of_death            0.692      0.057      0.213        159       5535\n",
      "profession                0.774      0.166      0.447        247       5623\n",
      "genre                     0.690      0.171      0.429        170       5546\n",
      "capital                   0.500      0.168      0.359         95       5471\n",
      "place_of_birth            0.836      0.197      0.508        233       5609\n",
      "contains                  0.799      0.573      0.740       3904       9280\n",
      "film_performance          0.820      0.483      0.720        766       6142\n",
      "adjoins                   0.886      0.344      0.674        340       5716\n",
      "is_a                      0.758      0.183      0.466        497       5873\n",
      "has_sibling               0.864      0.204      0.525        499       5875\n",
      "founders                  0.824      0.295      0.606        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.781      0.264      0.534       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[bigrams_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.692      0.209      0.474        301       5677\n",
      "author                    0.824      0.580      0.760        509       5885\n",
      "worked_at                 0.736      0.219      0.500        242       5618\n",
      "parents                   0.912      0.433      0.747        312       5688\n",
      "has_spouse                0.910      0.306      0.653        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.781      0.231      0.529        247       5623\n",
      "genre                     0.750      0.282      0.563        170       5546\n",
      "capital                   0.590      0.242      0.458         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.763      0.747      0.759       3904       9280\n",
      "film_performance          0.848      0.555      0.767        766       6142\n",
      "adjoins                   0.848      0.362      0.668        340       5716\n",
      "is_a                      0.758      0.252      0.540        497       5873\n",
      "has_sibling               0.883      0.226      0.559        499       5875\n",
      "founders                  0.844      0.342      0.653        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.791      0.333      0.594       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=True, directional=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.698      0.199      0.465        301       5677\n",
      "author                    0.825      0.574      0.758        509       5885\n",
      "worked_at                 0.718      0.211      0.485        242       5618\n",
      "parents                   0.915      0.417      0.739        312       5688\n",
      "has_spouse                0.914      0.288      0.637        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.759      0.259      0.547        170       5546\n",
      "capital                   0.579      0.232      0.445         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.838      0.627      0.785       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.867      0.365      0.680        340       5716\n",
      "is_a                      0.764      0.247      0.539        497       5873\n",
      "has_sibling               0.897      0.226      0.563        499       5875\n",
      "founders                  0.855      0.326      0.646        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.799      0.317      0.590       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=False, directional=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.698      0.199      0.465        301       5677\n",
      "author                    0.825      0.574      0.758        509       5885\n",
      "worked_at                 0.718      0.211      0.485        242       5618\n",
      "parents                   0.915      0.417      0.739        312       5688\n",
      "has_spouse                0.914      0.288      0.637        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.759      0.259      0.547        170       5546\n",
      "capital                   0.579      0.232      0.445         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.838      0.627      0.785       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.867      0.365      0.680        340       5716\n",
      "is_a                      0.764      0.247      0.539        497       5873\n",
      "has_sibling               0.897      0.226      0.563        499       5875\n",
      "founders                  0.855      0.326      0.646        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.799      0.317      0.590       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=False, use_right=True, directional=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.667      0.173      0.424        301       5677\n",
      "author                    0.750      0.519      0.689        509       5885\n",
      "worked_at                 0.761      0.211      0.500        242       5618\n",
      "parents                   0.868      0.420      0.715        312       5688\n",
      "has_spouse                0.909      0.301      0.648        594       5970\n",
      "place_of_death            0.412      0.044      0.154        159       5535\n",
      "profession                0.729      0.206      0.484        247       5623\n",
      "genre                     0.685      0.218      0.479        170       5546\n",
      "capital                   0.528      0.200      0.397         95       5471\n",
      "place_of_birth            0.806      0.215      0.520        233       5609\n",
      "contains                  0.786      0.587      0.736       3904       9280\n",
      "film_performance          0.814      0.516      0.730        766       6142\n",
      "adjoins                   0.847      0.341      0.653        340       5716\n",
      "is_a                      0.729      0.227      0.506        497       5873\n",
      "has_sibling               0.846      0.208      0.525        499       5875\n",
      "founders                  0.802      0.342      0.632        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.746      0.295      0.549       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.662      0.163      0.410        301       5677\n",
      "author                    0.759      0.513      0.692        509       5885\n",
      "worked_at                 0.731      0.202      0.480        242       5618\n",
      "parents                   0.882      0.407      0.715        312       5688\n",
      "has_spouse                0.911      0.291      0.639        594       5970\n",
      "place_of_death            0.500      0.050      0.179        159       5535\n",
      "profession                0.750      0.194      0.477        247       5623\n",
      "genre                     0.706      0.212      0.481        170       5546\n",
      "capital                   0.529      0.189      0.390         95       5471\n",
      "place_of_birth            0.817      0.210      0.518        233       5609\n",
      "contains                  0.791      0.582      0.738       3904       9280\n",
      "film_performance          0.820      0.512      0.732        766       6142\n",
      "adjoins                   0.854      0.344      0.659        340       5716\n",
      "is_a                      0.729      0.211      0.489        497       5873\n",
      "has_sibling               0.850      0.204      0.521        499       5875\n",
      "founders                  0.807      0.318      0.617        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.756      0.288      0.546       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=False)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.662      0.163      0.410        301       5677\n",
      "author                    0.759      0.513      0.692        509       5885\n",
      "worked_at                 0.731      0.202      0.480        242       5618\n",
      "parents                   0.882      0.407      0.715        312       5688\n",
      "has_spouse                0.911      0.291      0.639        594       5970\n",
      "place_of_death            0.500      0.050      0.179        159       5535\n",
      "profession                0.750      0.194      0.477        247       5623\n",
      "genre                     0.706      0.212      0.481        170       5546\n",
      "capital                   0.529      0.189      0.390         95       5471\n",
      "place_of_birth            0.817      0.210      0.518        233       5609\n",
      "contains                  0.791      0.582      0.738       3904       9280\n",
      "film_performance          0.820      0.512      0.732        766       6142\n",
      "adjoins                   0.854      0.344      0.659        340       5716\n",
      "is_a                      0.729      0.211      0.489        497       5873\n",
      "has_sibling               0.850      0.204      0.521        499       5875\n",
      "founders                  0.807      0.318      0.617        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.756      0.288      0.546       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=False, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.732      0.136      0.390        301       5677\n",
      "author                    0.777      0.458      0.682        509       5885\n",
      "worked_at                 0.811      0.124      0.385        242       5618\n",
      "parents                   0.925      0.276      0.629        312       5688\n",
      "has_spouse                0.940      0.239      0.593        594       5970\n",
      "place_of_death            0.714      0.031      0.134        159       5535\n",
      "profession                0.742      0.093      0.310        247       5623\n",
      "genre                     0.857      0.141      0.426        170       5546\n",
      "capital                   0.423      0.116      0.276         95       5471\n",
      "place_of_birth            0.800      0.103      0.340        233       5609\n",
      "contains                  0.808      0.478      0.710       3904       9280\n",
      "film_performance          0.789      0.381      0.650        766       6142\n",
      "adjoins                   0.884      0.315      0.649        340       5716\n",
      "is_a                      0.831      0.109      0.357        497       5873\n",
      "has_sibling               0.859      0.134      0.413        499       5875\n",
      "founders                  0.806      0.208      0.512        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.794      0.209      0.466       9248      95264\n"
     ]
    }
   ],
   "source": [
    "trigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[trigrams_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_bag_of_words_featurizer_use_middle = partial(ngrams_bag_of_words_featurizer, n=2, use_middle_length=True)\n",
    "bigrams_bag_of_words_featurizer_use_mid_direction = partial(ngrams_bag_of_words_featurizer, n=2, directional=True, use_middle_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.818      0.470      0.713        766       6142\n",
      "place_of_birth            0.692      0.193      0.456        233       5609\n",
      "nationality               0.592      0.140      0.359        301       5677\n",
      "place_of_death            0.467      0.044      0.160        159       5535\n",
      "has_spouse                0.874      0.256      0.589        594       5970\n",
      "genre                     0.738      0.182      0.459        170       5546\n",
      "founders                  0.766      0.276      0.566        380       5756\n",
      "has_sibling               0.890      0.178      0.495        499       5875\n",
      "is_a                      0.731      0.175      0.447        497       5873\n",
      "profession                0.684      0.158      0.411        247       5623\n",
      "worked_at                 0.682      0.186      0.445        242       5618\n",
      "contains                  0.808      0.548      0.738       3904       9280\n",
      "author                    0.777      0.458      0.682        509       5885\n",
      "capital                   0.750      0.158      0.429         95       5471\n",
      "adjoins                   0.925      0.326      0.677        340       5716\n",
      "parents                   0.857      0.385      0.688        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.753      0.258      0.519       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[bigrams_bag_of_words_featurizer_use_middle],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.692      0.209      0.474        301       5677\n",
      "author                    0.824      0.580      0.760        509       5885\n",
      "worked_at                 0.736      0.219      0.500        242       5618\n",
      "parents                   0.912      0.433      0.747        312       5688\n",
      "has_spouse                0.910      0.306      0.653        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.781      0.231      0.529        247       5623\n",
      "genre                     0.750      0.282      0.563        170       5546\n",
      "capital                   0.590      0.242      0.458         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.763      0.747      0.759       3904       9280\n",
      "film_performance          0.848      0.555      0.767        766       6142\n",
      "adjoins                   0.848      0.362      0.668        340       5716\n",
      "is_a                      0.758      0.252      0.540        497       5873\n",
      "has_sibling               0.883      0.226      0.559        499       5875\n",
      "founders                  0.844      0.342      0.653        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.791      0.333      0.594       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                        use_left=True, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.713      0.223      0.495        301       5677\n",
      "author                    0.853      0.572      0.777        509       5885\n",
      "worked_at                 0.803      0.202      0.504        242       5618\n",
      "parents                   0.915      0.413      0.736        312       5688\n",
      "has_spouse                0.877      0.288      0.622        594       5970\n",
      "place_of_death            0.609      0.088      0.279        159       5535\n",
      "profession                0.800      0.227      0.531        247       5623\n",
      "genre                     0.754      0.288      0.570        170       5546\n",
      "capital                   0.618      0.221      0.455         95       5471\n",
      "place_of_birth            0.864      0.219      0.544        233       5609\n",
      "contains                  0.786      0.688      0.764       3904       9280\n",
      "film_performance          0.850      0.554      0.768        766       6142\n",
      "adjoins                   0.863      0.353      0.670        340       5716\n",
      "is_a                      0.800      0.233      0.539        497       5873\n",
      "has_sibling               0.903      0.204      0.536        499       5875\n",
      "founders                  0.871      0.337      0.661        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.805      0.319      0.591       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                         use_middle_length=True, use_left=True, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.707      0.176      0.441        301       5677\n",
      "author                    0.858      0.558      0.775        509       5885\n",
      "worked_at                 0.754      0.190      0.473        242       5618\n",
      "parents                   0.922      0.417      0.742        312       5688\n",
      "has_spouse                0.881      0.286      0.622        594       5970\n",
      "place_of_death            0.619      0.082      0.267        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.784      0.235      0.535        170       5546\n",
      "capital                   0.576      0.200      0.419         95       5471\n",
      "place_of_birth            0.864      0.219      0.544        233       5609\n",
      "contains                  0.786      0.688      0.764       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.875      0.350      0.673        340       5716\n",
      "is_a                      0.803      0.221      0.526        497       5873\n",
      "has_sibling               0.904      0.206      0.539        499       5875\n",
      "founders                  0.883      0.318      0.652        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.803      0.307      0.578       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                         use_middle_length=True, use_left=False, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.707      0.176      0.441        301       5677\n",
      "author                    0.858      0.558      0.775        509       5885\n",
      "worked_at                 0.754      0.190      0.473        242       5618\n",
      "parents                   0.922      0.417      0.742        312       5688\n",
      "has_spouse                0.881      0.286      0.622        594       5970\n",
      "place_of_death            0.619      0.082      0.267        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.784      0.235      0.535        170       5546\n",
      "capital                   0.576      0.200      0.419         95       5471\n",
      "place_of_birth            0.864      0.219      0.544        233       5609\n",
      "contains                  0.786      0.688      0.764       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.875      0.350      0.673        340       5716\n",
      "is_a                      0.803      0.221      0.526        497       5873\n",
      "has_sibling               0.904      0.206      0.539        499       5875\n",
      "founders                  0.883      0.318      0.652        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.803      0.307      0.578       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                         use_middle_length=True, use_left=True, use_right=False)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.827      0.551      0.752        766       6142\n",
      "place_of_birth            0.724      0.236      0.512        233       5609\n",
      "nationality               0.678      0.266      0.517        301       5677\n",
      "place_of_death            0.559      0.119      0.322        159       5535\n",
      "has_spouse                0.931      0.295      0.650        594       5970\n",
      "genre                     0.829      0.371      0.665        170       5546\n",
      "founders                  0.794      0.376      0.650        380       5756\n",
      "has_sibling               0.930      0.240      0.591        499       5875\n",
      "is_a                      0.812      0.348      0.641        497       5873\n",
      "profession                0.800      0.340      0.630        247       5623\n",
      "worked_at                 0.744      0.277      0.556        242       5618\n",
      "contains                  0.783      0.379      0.645       3904       9280\n",
      "author                    0.794      0.538      0.725        509       5885\n",
      "capital                   0.714      0.158      0.419         95       5471\n",
      "adjoins                   0.884      0.426      0.728        340       5716\n",
      "parents                   0.859      0.545      0.770        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.791      0.342      0.611       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_entities_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True)\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_entities_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.769      0.453      0.675        766       6142\n",
      "place_of_birth            0.435      0.086      0.240        233       5609\n",
      "nationality               0.451      0.136      0.308        301       5677\n",
      "place_of_death            0.130      0.019      0.060        159       5535\n",
      "has_spouse                0.653      0.133      0.366        594       5970\n",
      "genre                     0.706      0.212      0.481        170       5546\n",
      "founders                  0.522      0.092      0.270        380       5756\n",
      "has_sibling               0.783      0.253      0.551        499       5875\n",
      "is_a                      0.642      0.282      0.511        497       5873\n",
      "profession                0.800      0.211      0.513        247       5623\n",
      "worked_at                 0.659      0.120      0.347        242       5618\n",
      "contains                  0.703      0.440      0.628       3904       9280\n",
      "author                    0.689      0.330      0.566        509       5885\n",
      "capital                   0.515      0.179      0.374         95       5471\n",
      "adjoins                   0.867      0.403      0.705        340       5716\n",
      "parents                   0.561      0.103      0.296        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.618      0.216      0.431       9248      95264\n"
     ]
    }
   ],
   "source": [
    "left_bag_of_words_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True, context_section='left')\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[left_bag_of_words_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.729      0.386      0.619        766       6142\n",
      "place_of_birth            0.326      0.060      0.173        233       5609\n",
      "nationality               0.400      0.100      0.250        301       5677\n",
      "place_of_death            0.421      0.050      0.170        159       5535\n",
      "has_spouse                0.537      0.121      0.319        594       5970\n",
      "genre                     0.750      0.212      0.497        170       5546\n",
      "founders                  0.517      0.082      0.250        380       5756\n",
      "has_sibling               0.676      0.146      0.392        499       5875\n",
      "is_a                      0.619      0.219      0.454        497       5873\n",
      "profession                0.765      0.158      0.432        247       5623\n",
      "worked_at                 0.526      0.083      0.254        242       5618\n",
      "contains                  0.682      0.419      0.606       3904       9280\n",
      "author                    0.632      0.240      0.476        509       5885\n",
      "capital                   0.465      0.211      0.375         95       5471\n",
      "adjoins                   0.914      0.409      0.733        340       5716\n",
      "parents                   0.578      0.119      0.326        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.596      0.188      0.395       9248      95264\n"
     ]
    }
   ],
   "source": [
    "right_bag_of_words_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True, context_section='right')\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[right_bag_of_words_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.751      0.617      0.720        766       6142\n",
      "place_of_birth            0.562      0.253      0.452        233       5609\n",
      "nationality               0.521      0.326      0.465        301       5677\n",
      "place_of_death            0.383      0.145      0.288        159       5535\n",
      "has_spouse                0.872      0.332      0.658        594       5970\n",
      "genre                     0.764      0.553      0.710        170       5546\n",
      "founders                  0.803      0.418      0.678        380       5756\n",
      "has_sibling               0.884      0.305      0.640        499       5875\n",
      "is_a                      0.742      0.493      0.674        497       5873\n",
      "profession                0.756      0.478      0.677        247       5623\n",
      "worked_at                 0.692      0.343      0.575        242       5618\n",
      "contains                  0.778      0.426      0.668       3904       9280\n",
      "author                    0.788      0.585      0.737        509       5885\n",
      "capital                   0.553      0.221      0.425         95       5471\n",
      "adjoins                   0.817      0.474      0.714        340       5716\n",
      "parents                   0.832      0.571      0.762        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.719      0.409      0.615       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_middle_entities_featurizer = partial(simple_bag_of_words_featurizer, \n",
    "                                                         use_entities=True, use_middle_length=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_middle_entities_featurizer],\n",
    "    model_factory=svc_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer2(kbt, corpus, feature_counter, \n",
    "                                    use_middle_length=False, \n",
    "                                    use_entities=False,\n",
    "                                    context_section='middle', # can be 'left', 'right', or 'middle'\n",
    "                                    use_synsets=False):\n",
    "    synset_prefix = \"synset_:\"\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "        \n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos_pair in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[word] += 1\n",
    "                        for hyponym in syn.hyponyms():\n",
    "                            feature_counter[synset_prefix+hyponym.name()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        \n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "\n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos_pair in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[word] += 1\n",
    "                        for hyponym in syn.hyponyms():\n",
    "                            feature_counter[synset_prefix+hyponym.name()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.698      0.615      0.679        766       6142\n",
      "place_of_birth            0.460      0.223      0.380        233       5609\n",
      "nationality               0.374      0.243      0.338        301       5677\n",
      "place_of_death            0.329      0.145      0.262        159       5535\n",
      "has_spouse                0.795      0.327      0.618        594       5970\n",
      "genre                     0.400      0.306      0.377        170       5546\n",
      "founders                  0.630      0.447      0.582        380       5756\n",
      "has_sibling               0.730      0.261      0.537        499       5875\n",
      "is_a                      0.496      0.262      0.421        497       5873\n",
      "profession                0.458      0.219      0.376        247       5623\n",
      "worked_at                 0.565      0.306      0.483        242       5618\n",
      "contains                  0.755      0.593      0.716       3904       9280\n",
      "author                    0.704      0.603      0.681        509       5885\n",
      "capital                   0.556      0.263      0.455         95       5471\n",
      "adjoins                   0.649      0.326      0.542        340       5716\n",
      "parents                   0.776      0.545      0.715        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.586      0.355      0.510       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_synsets_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                                         use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_synsets_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.738      0.598      0.705        766       6142\n",
      "place_of_birth            0.481      0.219      0.388        233       5609\n",
      "nationality               0.484      0.306      0.434        301       5677\n",
      "place_of_death            0.403      0.157      0.307        159       5535\n",
      "has_spouse                0.877      0.300      0.633        594       5970\n",
      "genre                     0.574      0.412      0.532        170       5546\n",
      "founders                  0.647      0.400      0.576        380       5756\n",
      "has_sibling               0.772      0.244      0.539        499       5875\n",
      "is_a                      0.676      0.356      0.573        497       5873\n",
      "profession                0.730      0.328      0.586        247       5623\n",
      "worked_at                 0.615      0.277      0.494        242       5618\n",
      "contains                  0.768      0.472      0.682       3904       9280\n",
      "author                    0.767      0.587      0.723        509       5885\n",
      "capital                   0.629      0.232      0.468         95       5471\n",
      "adjoins                   0.671      0.324      0.552        340       5716\n",
      "parents                   0.779      0.519      0.708        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.663      0.358      0.556       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_all_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                            use_entities=True, use_middle_length=True, use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_all_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.738      0.598      0.705        766       6142\n",
      "place_of_birth            0.481      0.219      0.388        233       5609\n",
      "nationality               0.484      0.306      0.434        301       5677\n",
      "place_of_death            0.403      0.157      0.307        159       5535\n",
      "has_spouse                0.877      0.300      0.633        594       5970\n",
      "genre                     0.574      0.412      0.532        170       5546\n",
      "founders                  0.647      0.400      0.576        380       5756\n",
      "has_sibling               0.772      0.244      0.539        499       5875\n",
      "is_a                      0.676      0.356      0.573        497       5873\n",
      "profession                0.730      0.328      0.586        247       5623\n",
      "worked_at                 0.615      0.277      0.494        242       5618\n",
      "contains                  0.768      0.472      0.682       3904       9280\n",
      "author                    0.767      0.587      0.723        509       5885\n",
      "capital                   0.629      0.232      0.468         95       5471\n",
      "adjoins                   0.671      0.324      0.552        340       5716\n",
      "parents                   0.779      0.519      0.708        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.663      0.358      0.556       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_all_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                            use_entities=True, use_middle_length=True, use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_all_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.847      0.645      0.797        766       6142\n",
      "place_of_birth            0.692      0.232      0.495        233       5609\n",
      "nationality               0.623      0.219      0.455        301       5677\n",
      "place_of_death            0.550      0.138      0.345        159       5535\n",
      "has_spouse                0.896      0.347      0.680        594       5970\n",
      "genre                     0.657      0.259      0.502        170       5546\n",
      "founders                  0.823      0.416      0.688        380       5756\n",
      "has_sibling               0.897      0.244      0.585        499       5875\n",
      "is_a                      0.747      0.225      0.510        497       5873\n",
      "profession                0.740      0.231      0.514        247       5623\n",
      "worked_at                 0.727      0.264      0.539        242       5618\n",
      "contains                  0.845      0.652      0.798       3904       9280\n",
      "author                    0.838      0.589      0.773        509       5885\n",
      "capital                   0.677      0.221      0.479         95       5471\n",
      "adjoins                   0.890      0.403      0.717        340       5716\n",
      "parents                   0.848      0.519      0.753        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.769      0.350      0.602       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_middle_featurizer = partial(directional_bag_of_words_featurizer, use_middle_length=True)\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_middle_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.855      0.633      0.799        766       6142\n",
      "place_of_birth            0.772      0.262      0.556        233       5609\n",
      "nationality               0.705      0.326      0.572        301       5677\n",
      "place_of_death            0.565      0.164      0.379        159       5535\n",
      "has_spouse                0.944      0.311      0.671        594       5970\n",
      "genre                     0.873      0.406      0.710        170       5546\n",
      "founders                  0.828      0.379      0.669        380       5756\n",
      "has_sibling               0.947      0.251      0.609        499       5875\n",
      "is_a                      0.811      0.372      0.656        497       5873\n",
      "profession                0.828      0.389      0.675        247       5623\n",
      "worked_at                 0.756      0.281      0.565        242       5618\n",
      "contains                  0.848      0.414      0.701       3904       9280\n",
      "author                    0.856      0.621      0.796        509       5885\n",
      "capital                   0.697      0.242      0.507         95       5471\n",
      "adjoins                   0.936      0.385      0.728        340       5716\n",
      "parents                   0.859      0.510      0.756        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.818      0.372      0.647       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_middle_entities_featurizer = partial(directional_bag_of_words_featurizer, \n",
    "                                                 use_middle_length=True, use_entities=True)\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_middle_entities_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True) \n",
    "    return bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.877      0.664      0.824        301       5677\n",
      "author                    0.878      0.723      0.842        509       5885\n",
      "worked_at                 0.777      0.302      0.591        242       5618\n",
      "parents                   0.881      0.667      0.828        312       5688\n",
      "has_spouse                0.822      0.646      0.780        594       5970\n",
      "place_of_death            0.779      0.421      0.666        159       5535\n",
      "profession                0.917      0.534      0.802        247       5623\n",
      "genre                     0.949      0.329      0.690        170       5546\n",
      "capital                   0.462      0.126      0.302         95       5471\n",
      "place_of_birth            0.841      0.498      0.739        233       5609\n",
      "contains                  0.865      0.690      0.823       3904       9280\n",
      "film_performance          0.858      0.696      0.820        766       6142\n",
      "adjoins                   0.817      0.315      0.619        340       5716\n",
      "is_a                      0.869      0.559      0.782        497       5873\n",
      "has_sibling               0.853      0.651      0.803        499       5875\n",
      "founders                  0.732      0.295      0.565        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.824      0.507      0.717       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_ngrams_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True) \n",
    "    return bigrams_bag_of_words_featurizer(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.898      0.645      0.833        301       5677\n",
      "author                    0.872      0.721      0.837        509       5885\n",
      "worked_at                 0.813      0.252      0.563        242       5618\n",
      "parents                   0.889      0.667      0.833        312       5688\n",
      "has_spouse                0.846      0.603      0.783        594       5970\n",
      "place_of_death            0.847      0.384      0.682        159       5535\n",
      "profession                0.923      0.486      0.782        247       5623\n",
      "genre                     0.925      0.288      0.641        170       5546\n",
      "capital                   0.480      0.126      0.308         95       5471\n",
      "place_of_birth            0.895      0.438      0.740        233       5609\n",
      "contains                  0.870      0.674      0.822       3904       9280\n",
      "film_performance          0.870      0.688      0.826        766       6142\n",
      "adjoins                   0.836      0.285      0.603        340       5716\n",
      "is_a                      0.881      0.549      0.786        497       5873\n",
      "has_sibling               0.870      0.629      0.808        499       5875\n",
      "founders                  0.742      0.311      0.581        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.841      0.484      0.714       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_ngrams_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_ngrams_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_featurizer2(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    return bigrams_bag_of_words_featurizer(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.858      0.692      0.818        766       6142\n",
      "place_of_birth            0.779      0.468      0.687        233       5609\n",
      "nationality               0.871      0.648      0.815        301       5677\n",
      "place_of_death            0.739      0.409      0.636        159       5535\n",
      "has_spouse                0.836      0.616      0.780        594       5970\n",
      "genre                     0.908      0.347      0.686        170       5546\n",
      "founders                  0.825      0.458      0.711        380       5756\n",
      "has_sibling               0.909      0.637      0.837        499       5875\n",
      "is_a                      0.861      0.547      0.772        497       5873\n",
      "profession                0.899      0.543      0.795        247       5623\n",
      "worked_at                 0.758      0.285      0.569        242       5618\n",
      "contains                  0.857      0.689      0.817       3904       9280\n",
      "author                    0.857      0.750      0.833        509       5885\n",
      "capital                   0.636      0.147      0.383         95       5471\n",
      "adjoins                   0.797      0.335      0.625        340       5716\n",
      "parents                   0.867      0.692      0.826        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.828      0.516      0.724       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_featurizer2],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_direct_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    return bigrams_bag_of_words_featurizer_use_mid_direction(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.904      0.654      0.840        301       5677\n",
      "author                    0.889      0.741      0.855        509       5885\n",
      "worked_at                 0.760      0.314      0.592        242       5618\n",
      "parents                   0.882      0.673      0.831        312       5688\n",
      "has_spouse                0.820      0.650      0.779        594       5970\n",
      "place_of_death            0.764      0.428      0.660        159       5535\n",
      "profession                0.908      0.522      0.791        247       5623\n",
      "genre                     0.983      0.347      0.720        170       5546\n",
      "capital                   0.429      0.126      0.290         95       5471\n",
      "place_of_birth            0.837      0.506      0.740        233       5609\n",
      "contains                  0.865      0.701      0.826       3904       9280\n",
      "film_performance          0.860      0.706      0.824        766       6142\n",
      "adjoins                   0.805      0.303      0.604        340       5716\n",
      "is_a                      0.860      0.557      0.776        497       5873\n",
      "has_sibling               0.858      0.663      0.810        499       5875\n",
      "founders                  0.806      0.471      0.706        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.827      0.523      0.728       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_direct_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_direct_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter)\n",
    "    return bigrams_bag_of_words_featurizer(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.902      0.641      0.834        301       5677\n",
      "author                    0.889      0.741      0.855        509       5885\n",
      "worked_at                 0.784      0.285      0.581        242       5618\n",
      "parents                   0.887      0.683      0.837        312       5688\n",
      "has_spouse                0.844      0.638      0.793        594       5970\n",
      "place_of_death            0.816      0.390      0.670        159       5535\n",
      "profession                0.903      0.530      0.792        247       5623\n",
      "genre                     0.967      0.341      0.707        170       5546\n",
      "capital                   0.520      0.137      0.333         95       5471\n",
      "place_of_birth            0.827      0.451      0.709        233       5609\n",
      "contains                  0.868      0.688      0.825       3904       9280\n",
      "film_performance          0.868      0.705      0.830        766       6142\n",
      "adjoins                   0.824      0.303      0.613        340       5716\n",
      "is_a                      0.868      0.557      0.781        497       5873\n",
      "has_sibling               0.860      0.651      0.808        499       5875\n",
      "founders                  0.806      0.471      0.706        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.840      0.513      0.730       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_final(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    return ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, directional=True, \n",
    "                                        use_left=True, use_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.902      0.645      0.835        301       5677\n",
      "author                    0.893      0.741      0.858        509       5885\n",
      "worked_at                 0.735      0.310      0.577        242       5618\n",
      "parents                   0.879      0.673      0.828        312       5688\n",
      "has_spouse                0.827      0.653      0.785        594       5970\n",
      "place_of_death            0.802      0.434      0.686        159       5535\n",
      "profession                0.907      0.551      0.803        247       5623\n",
      "genre                     0.983      0.341      0.714        170       5546\n",
      "capital                   0.464      0.137      0.314         95       5471\n",
      "place_of_birth            0.846      0.494      0.740        233       5609\n",
      "contains                  0.866      0.709      0.829       3904       9280\n",
      "film_performance          0.864      0.706      0.827        766       6142\n",
      "adjoins                   0.789      0.297      0.593        340       5716\n",
      "is_a                      0.854      0.555      0.771        497       5873\n",
      "has_sibling               0.857      0.659      0.808        499       5875\n",
      "founders                  0.803      0.492      0.713        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.829      0.525      0.730       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_direct_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_final],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_bag_of_words_featurizer(kbt, corpus, feature_counter, glove_lookup,\n",
    "                                context_section='middle',\n",
    "                                use_middle_length=False,\n",
    "                                glove_dims=300): # can be 'left', 'right', or 'middle'\n",
    "    fwd_glove_vector = np.zeros(glove_dims)\n",
    "    bwd_glove_vector = np.zeros(glove_dims)\n",
    "\n",
    "    sbj_glove = glove_lookup.get(kbt.sbj, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))\n",
    "    obj_glove = glove_lookup.get(kbt.obj, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))\n",
    "\n",
    "    feature_prefix = \"sbj_glove:\"\n",
    "    for i, feature in enumerate(sbj_glove):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "        \n",
    "    feature_prefix = \"obj_glove:\"\n",
    "    for i, feature in enumerate(obj_glove):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            #words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "            words = ' '.join((ex.mention_1, ex.middle, ex.mention_2)).split(' ')\n",
    "        for word in words:\n",
    "            fwd_glove_vector += glove_lookup.get(word, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))/len(words)\n",
    "        if use_middle_length:\n",
    "            feature_counter['FWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            #words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "            words = ' '.join((ex.mention_1, ex.middle, ex.mention_2)).split(' ')\n",
    "        for word in words:\n",
    "            bwd_glove_vector += glove_lookup.get(word, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))/len(words)\n",
    "        if use_middle_length:\n",
    "            feature_counter['BWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "    \n",
    "    feature_prefix = \"fwd_glove_:\"\n",
    "    for i, feature in enumerate(fwd_glove_vector):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "\n",
    "    feature_prefix = \"bwd_glove_:\"\n",
    "    for i, feature in enumerate(bwd_glove_vector):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "    \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"glove_featurizer = partial(glove_bag_of_words_featurizer, context_section='all', glove_lookup=glove_lookup)\\n\\nglove_results = rel_ext.experiment(\\n    splits,\\n    train_split='train',\\n    test_split='dev',\\n    featurizers=[glove_featurizer],\\n    model_factory=model_factory_2k,\\n    verbose=True)\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''glove_featurizer = partial(glove_bag_of_words_featurizer, context_section='all', glove_lookup=glove_lookup)\n",
    "\n",
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[glove_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_length_featurizer = partial(glove_bag_of_words_featurizer, \n",
    "                                  context_section='all', \n",
    "                                  use_middle_length=True, glove_lookup=glove_lookup)\n",
    "\n",
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[glove_length_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_glove_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    feature_counter =  ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, directional=True, \n",
    "                                        use_left=True, use_right=True)\n",
    "    return glove_bag_of_words_featurizer(kbt, corpus, feature_counter, \n",
    "                                         context_section='all', glove_lookup=glove_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_glove_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('notebook_env.db')\n",
    "#dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release a test set right after class on April 29. The announcement will go out on Piazza. You will evaluate your custom model from the previous question on these new datasets using the function `rel_ext.bake_off_experiment`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187248\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on May 1. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average f-score (an F_0.5 score) as reported \n",
    "# by the code above. Please enter only a number between \n",
    "# 0 and 1 inclusive. Please do not remove this comment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
