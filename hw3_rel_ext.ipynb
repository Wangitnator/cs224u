{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney\"\n",
    "__version__ = \"CS224U, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Baseline](#Baseline)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Different model factory [1 point]](#Different-model-factory-[1-point])\n",
    "  1. [Directional unigram features [2 points]](#Directional-unigram-features-[2-points])\n",
    "  1. [The part-of-speech tags of the \"middle\" words [2 points]](#The-part-of-speech-tags-of-the-\"middle\"-words-[2-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to the developing really effective relation extraction systems using distant supervision. \n",
    "\n",
    "As with the previous assignments, this notebook first establishes a baseline system. The initial homework questions ask you to create additional baselines and suggest areas for innovation, and the final homework question asks you to develop an original system for you to enter into the bake-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import rel_ext\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE\n",
      "obt\n"
     ]
    }
   ],
   "source": [
    "sbt_obj = \"Stockholm\"\n",
    "obt_obj = \"Bergen\"\n",
    "#feature_counter[\"sbt_obj\"] = 0\n",
    "#feature_counter[\"obt_obj\"] = 0\n",
    "text = nltk.word_tokenize(sbt_obj)\n",
    "nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "for ne in nes:\n",
    "    if type(ne) is nltk.tree.Tree and ne.label() == \"GPE\":\n",
    "        print(ne.label())\n",
    "        #feature_counter[\"sbt_obj\"] = 1\n",
    "\n",
    "text = nltk.word_tokenize(obt_obj)\n",
    "nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "for ne in nes:\n",
    "    if type(ne) is nltk.tree.Tree and ne.label() == \"GPE\":\n",
    "        print(\"obt\")\n",
    "        #feature_counter[\"obt_obj\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we unite our corpus and KB into a dataset, and create some splits for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_HOME = '/home/kd/data/data'\n",
    "rel_ext_data_home = os.path.join(DATA_HOME, 'rel_ext_data')\n",
    "GLOVE_HOME = os.path.join(DATA_HOME, 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not wedded to this set-up for splits. The bake-off will be conducted on a previously unseen test-set, so all of the data in `dataset` is fair game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.79, 0.20],\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': Corpus with 331,696 examples; KB with 45,884 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 263,285 examples; KB with 36,191 triples}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter, \n",
    "                                    use_middle_length=False, \n",
    "                                    use_entities=False,\n",
    "                                    context_section='middle', # can be 'left', 'right', or 'middle'\n",
    "                                    use_synsets=False):\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "        \n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word, pos_word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[syn.lemma()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        \n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "\n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word, pos_word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[syn.lemma()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "featurizers = [simple_bag_of_words_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')\n",
    "model_factory_2k = lambda: LogisticRegression(fit_intercept=True, solver='liblinear', max_iter=2000)\n",
    "model_factory_4k = lambda: LogisticRegression(fit_intercept=True, solver='liblinear', max_iter=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "contains                  0.793      0.599      0.745       3904       9280\n",
      "film_performance          0.800      0.552      0.734        766       6142\n",
      "capital                   0.567      0.179      0.395         95       5471\n",
      "place_of_birth            0.706      0.206      0.475        233       5609\n",
      "parents                   0.852      0.535      0.762        312       5688\n",
      "is_a                      0.675      0.213      0.471        497       5873\n",
      "has_sibling               0.874      0.236      0.568        499       5875\n",
      "author                    0.776      0.511      0.703        509       5885\n",
      "has_spouse                0.864      0.322      0.646        594       5970\n",
      "worked_at                 0.700      0.231      0.498        242       5618\n",
      "place_of_death            0.607      0.107      0.314        159       5535\n",
      "nationality               0.654      0.169      0.416        301       5677\n",
      "profession                0.608      0.194      0.426        247       5623\n",
      "adjoins                   0.850      0.382      0.683        340       5716\n",
      "founders                  0.818      0.403      0.678        380       5756\n",
      "genre                     0.674      0.182      0.438        170       5546\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.739      0.314      0.560       9248      95264\n"
     ]
    }
   ],
   "source": [
    "baseline_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying model weights might yield insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-18f59039c34c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrel_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamine_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_results' is not defined"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different model factory [1 point]\n",
    "\n",
    "The code in `rel_ext` makes it very easy to experiment with other classifier models: one need only redefine the `model_factory` argument. This question asks you to assess a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "__To submit:__ A call to `rel_ext.experiment` training on the 'train' part of `splits` and assessing on its `dev` part, with `featurizers` as defined above in this notebook and the `model_factory` set to one based in an `SVC` with `kernel='linear'` and all other arguments left with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_factory = lambda: SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.752      0.627      0.723        766       6142\n",
      "place_of_birth            0.602      0.215      0.442        233       5609\n",
      "nationality               0.496      0.189      0.375        301       5677\n",
      "place_of_death            0.450      0.113      0.282        159       5535\n",
      "has_spouse                0.842      0.342      0.651        594       5970\n",
      "genre                     0.516      0.276      0.440        170       5546\n",
      "founders                  0.725      0.424      0.635        380       5756\n",
      "has_sibling               0.774      0.255      0.550        499       5875\n",
      "is_a                      0.608      0.284      0.495        497       5873\n",
      "profession                0.577      0.259      0.463        247       5623\n",
      "worked_at                 0.622      0.306      0.515        242       5618\n",
      "contains                  0.777      0.602      0.735       3904       9280\n",
      "author                    0.717      0.611      0.693        509       5885\n",
      "capital                   0.765      0.274      0.563         95       5471\n",
      "adjoins                   0.822      0.312      0.619        340       5716\n",
      "parents                   0.783      0.590      0.735        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.677      0.355      0.557       9248      95264\n"
     ]
    }
   ],
   "source": [
    "svc_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=svc_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional unigram features [2 points]\n",
    "\n",
    "The current bag-of-words representation makes no distinction between \"forward\" and \"reverse\" examples. But, intuitively, there is big difference between _X and his son Y_ and _Y and his son X_. This question asks you to modify `simple_bag_of_words_featurizer` to capture these differences. \n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `directional_bag_of_words_featurizer` that is just like `simple_bag_of_words_featurizer` except that it distinguishes \"forward\" and \"reverse\". To do this, you just need to mark each word feature for whether it is derived from a subject–object example or from an object–subject example. The precise nature of the mark you add for the two cases doesn't make a difference to the model.\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `directional_bag_of_words_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "3. `rel_ext.experiment` returns some of the core objects used in the experiment. How many feature names does the `vectorizer` have for the experiment run in the previous step? (Note: we're partly asking you to figure out how to get this value by using the sklearn documentation, so please don't ask how to do it on Piazza!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter, fwd_prefix='$FWD_DIRECTION: ', \n",
    "                                        bwd_prefix='$BWD_DIREECTION: ', use_middle_length=False,\n",
    "                                        use_entities=False, include_left=False, include_right=False,\n",
    "                                       use_entities2=False):\n",
    "    count = 0\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        count += 1\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            word_direction = fwd_prefix + \"_middle_\" + word\n",
    "            feature_counter[word_direction] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter[fwd_prefix + 'NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if include_left:\n",
    "            words = ex.left.split(' ')\n",
    "            for word in words:\n",
    "                word_key = fwd_prefix + \"_left_\" + word\n",
    "                feature_counter[word_key] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter[fwd_prefix + 'NUM_WORD_IN_LEFT']  += len(words)\n",
    "        if include_right:\n",
    "            words = ex.right.split(' ')\n",
    "            for word in words:\n",
    "                word_key = fwd_prefix + \"_right_\" + word\n",
    "                feature_counter[word_key] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter[fwd_prefix + 'NUM_WORD_IN_RIGHT']  += len(words)                \n",
    "        if use_entities:\n",
    "            feature_counter[fwd_prefix + kbt.sbj] += 1\n",
    "            feature_counter[fwd_prefix + kbt.obj] += 1\n",
    "        if use_entities2:\n",
    "            feature_counter[\"fwd_kbt.sbj\"] += 1\n",
    "            feature_counter[\"fwd_kbt.obj\"] += 1\n",
    "\n",
    "    count = max(count, 1)\n",
    "    if use_middle_length:\n",
    "        feature_counter[fwd_prefix + 'NUM_WORD_IN_MIDDLE']  /= count\n",
    "    if include_left:\n",
    "        feature_counter[fwd_prefix + 'NUM_WORD_IN_LEFT'] /= count\n",
    "    if include_right:\n",
    "        feature_counter[fwd_prefix + 'NUM_WORD_IN_RIGHT'] /= count\n",
    "        \n",
    "    count = 0\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        count += 1\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            word_direction = bwd_prefix +\"_middle_\" + word\n",
    "            feature_counter[word_direction] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['BWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "            \n",
    "        if include_left:\n",
    "            words = ex.left.split(' ')\n",
    "            for word in words:\n",
    "                word_key = bwd_prefix + \"_left_\" + word\n",
    "                feature_counter[word_key] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter['BWD_NUM_WORD_IN_LEFT']  += len(words)\n",
    "        if include_right:\n",
    "            words = ex.right.split(' ')\n",
    "            for word in words:\n",
    "                word_key = bwd_prefix + \"_right_\" + word\n",
    "                feature_counter[word_key] += 1\n",
    "            if use_middle_length:\n",
    "                feature_counter[bwd_prefix +\"BWD_NUM_WORD_IN_RIGHT'\"]  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[bwd_prefix + kbt.sbj] += 1\n",
    "            feature_counter[bwd_prefix + kbt.obj] += 1\n",
    "        if use_entities2:\n",
    "            feature_counter[\"bwd_kbt.sbj\"] += 1\n",
    "            feature_counter[\"bwd_kbt.obj\"] += 1\n",
    "            \n",
    "    count = max(count, 1)\n",
    "\n",
    "    if use_middle_length:\n",
    "        feature_counter[fwd_prefix + 'NUM_WORD_IN_MIDDLE']  /= count\n",
    "    if include_left:\n",
    "        feature_counter[fwd_prefix + 'NUM_WORD_IN_LEFT'] /= count\n",
    "    if include_right:\n",
    "        feature_counter[fwd_prefix + 'NUM_WORD_IN_RIGHT'] /= count\n",
    "\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "contains                  0.838      0.650      0.792       3904       9280\n",
      "film_performance          0.849      0.667      0.805        766       6142\n",
      "capital                   0.667      0.232      0.485         95       5471\n",
      "place_of_birth            0.687      0.245      0.504        233       5609\n",
      "parents                   0.862      0.519      0.761        312       5688\n",
      "is_a                      0.731      0.245      0.524        497       5873\n",
      "has_sibling               0.866      0.246      0.576        499       5875\n",
      "author                    0.826      0.568      0.757        509       5885\n",
      "has_spouse                0.872      0.343      0.667        594       5970\n",
      "worked_at                 0.776      0.273      0.567        242       5618\n",
      "place_of_death            0.511      0.151      0.346        159       5535\n",
      "nationality               0.638      0.223      0.465        301       5677\n",
      "profession                0.725      0.235      0.511        247       5623\n",
      "adjoins                   0.849      0.397      0.692        340       5716\n",
      "founders                  0.842      0.408      0.694        380       5756\n",
      "genre                     0.768      0.253      0.546        170       5546\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.769      0.353      0.606       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "founders                  0.774      0.476      0.688        380       5756\n",
      "has_spouse                0.825      0.657      0.784        594       5970\n",
      "place_of_birth            0.800      0.446      0.691        233       5609\n",
      "adjoins                   0.800      0.529      0.726        340       5716\n",
      "genre                     0.729      0.253      0.530        170       5546\n",
      "place_of_death            0.753      0.421      0.650        159       5535\n",
      "parents                   0.842      0.647      0.794        312       5688\n",
      "nationality               0.783      0.611      0.741        301       5677\n",
      "is_a                      0.826      0.479      0.722        497       5873\n",
      "worked_at                 0.814      0.343      0.638        242       5618\n",
      "profession                0.824      0.397      0.678        247       5623\n",
      "capital                   0.706      0.253      0.519         95       5471\n",
      "film_performance          0.849      0.692      0.812        766       6142\n",
      "author                    0.882      0.747      0.851        509       5885\n",
      "has_sibling               0.882      0.661      0.827        499       5875\n",
      "contains                  0.851      0.815      0.844       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.809      0.527      0.718       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_bag_left_right_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(directional_bag_of_words_featurizer, use_middle_length=True, \n",
    "                         use_entities=False, include_left=True, include_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "founders                  0.784      0.468      0.691        380       5756\n",
      "has_spouse                0.843      0.653      0.797        594       5970\n",
      "place_of_birth            0.824      0.442      0.703        233       5609\n",
      "adjoins                   0.808      0.521      0.728        340       5716\n",
      "genre                     0.789      0.265      0.565        170       5546\n",
      "place_of_death            0.786      0.415      0.667        159       5535\n",
      "parents                   0.854      0.638      0.800        312       5688\n",
      "nationality               0.835      0.638      0.786        301       5677\n",
      "is_a                      0.864      0.511      0.759        497       5873\n",
      "worked_at                 0.824      0.347      0.646        242       5618\n",
      "profession                0.903      0.453      0.754        247       5623\n",
      "capital                   0.636      0.221      0.463         95       5471\n",
      "film_performance          0.850      0.687      0.811        766       6142\n",
      "author                    0.895      0.739      0.859        509       5885\n",
      "has_sibling               0.892      0.663      0.835        499       5875\n",
      "contains                  0.858      0.710      0.824       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.828      0.523      0.730       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_bag_left_right_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(directional_bag_of_words_featurizer, use_middle_length=True, \n",
    "                         use_entities=True, include_left=True, include_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-afcdc4f4be1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                          use_entities2=True, include_left=True, include_right=True)],\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_factory_2k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     verbose=True)\n\u001b[0m",
      "\u001b[0;32m/data/home/kd/repos/cs224u/rel_ext.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(splits, featurizers, train_split, test_split, model_factory, verbose)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0msplit_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mmodel_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         verbose=verbose)\n\u001b[0m\u001b[1;32m    395\u001b[0m     predictions, test_y = predict(\n\u001b[1;32m    396\u001b[0m         \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/kd/repos/cs224u/rel_ext.py\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(splits, featurizers, split_name, model_factory, verbose)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_relations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     return {\n\u001b[1;32m    346\u001b[0m         \u001b[0;34m'featurizers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeaturizers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directional_bag_left_right_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(directional_bag_of_words_featurizer, use_middle_length=True, \n",
    "                         use_entities2=True, include_left=True, include_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_bag_left_right_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(directional_bag_of_words_featurizer, use_middle_length=True, use_entities=True, \n",
    "                         use_entities2=True, include_left=True, include_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The part-of-speech tags of the \"middle\" words [2 points]\n",
    "\n",
    "Our corpus distribution contains part-of-speech (POS) tagged versions of the core text spans. Let's begin to explore whether there is information in these sequences, focusing on `middle_POS`.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `middle_bigram_pos_tag_featurizer` that is just like `simple_bag_of_words_featurizer` except that it creates a feature for bigram POS sequences. For example, given \n",
    "\n",
    "  `The/DT dog/N napped/V`\n",
    "  \n",
    "   we obtain the list of bigram POS sequences\n",
    "  \n",
    "   `b = ['<s> DT', 'DT N', 'N V', 'V </s>']`. \n",
    "   \n",
    "   Of course, `middle_bigram_pos_tag_featurizer` should return count dictionaries defined in terms of such bigram POS lists, on the model of `simple_bag_of_words_featurizer`.\n",
    "   \n",
    "   Don't forget the start and end tags, to model those environments properly!\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `middle_bigram_pos_tag_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "Note: To parse `middle_POS`, one splits on whitespace to get the `word/TAG` pairs. Each of these pairs `s` can be parsed with `s.rsplit('/', 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_featurize(pos_segments, feature_counter, prefix=\"\"):\n",
    "    word_POSs = pos_segments.split(' ')\n",
    "    len_POS = len(word_POSs)\n",
    "    for i in range(-1, len_POS - 1):\n",
    "        pos = word_POSs[i].rsplit('/', 1)\n",
    "        bigram = \"\"\n",
    "        if len(pos) > 1:\n",
    "            if i == -1:\n",
    "                bigram = '<s> ' + pos[1]\n",
    "            elif i == len_POS - 2:\n",
    "                bigram = pos[1] + ' </s>'\n",
    "            else:\n",
    "                bigram = pos[1] + \" \" + word_POSs[i+1].rsplit('/', 1)[1]\n",
    "        feature_counter[prefix + bigram] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=False, use_right=False, use_bt_pos=False):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        if use_bt_pos:\n",
    "            mention_pos_1 = ex.mention_1_POS.rsplit('/',1)\n",
    "            mention_pos_2 = ex.mention_1_POS.rsplit('/',1)\n",
    "            feature_counter[\"sbj_\"+mention_pos_1[1]] = 1\n",
    "            feature_counter[\"obj_\"+mention_pos_1[1]] = 1\n",
    "        feature_counter = pos_featurize(ex.middle_POS, feature_counter, \"middle\")\n",
    "        if use_left:\n",
    "            feature_counter = pos_featurize(ex.left_POS, feature_counter, \"left\")\n",
    "        if use_right:\n",
    "            feature_counter = pos_featurize(ex.right_POS, feature_counter, \"right\")\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter)\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.653      0.163      0.408        301       5677\n",
      "author                    0.845      0.246      0.568        509       5885\n",
      "worked_at                 0.607      0.140      0.365        242       5618\n",
      "parents                   0.735      0.240      0.521        312       5688\n",
      "has_spouse                0.777      0.258      0.554        594       5970\n",
      "place_of_death            0.700      0.132      0.376        159       5535\n",
      "profession                0.742      0.186      0.465        247       5623\n",
      "genre                     0.824      0.082      0.294        170       5546\n",
      "capital                   0.500      0.095      0.269         95       5471\n",
      "place_of_birth            0.750      0.206      0.491        233       5609\n",
      "contains                  0.725      0.297      0.563       3904       9280\n",
      "film_performance          0.730      0.261      0.537        766       6142\n",
      "adjoins                   0.878      0.382      0.697        340       5716\n",
      "is_a                      0.672      0.161      0.411        497       5873\n",
      "has_sibling               0.743      0.168      0.442        499       5875\n",
      "founders                  0.697      0.139      0.387        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.724      0.197      0.459       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[middle_bigram_pos_tag_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.552      0.246      0.442        301       5677\n",
      "author                    0.717      0.379      0.609        509       5885\n",
      "worked_at                 0.481      0.260      0.411        242       5618\n",
      "parents                   0.667      0.269      0.515        312       5688\n",
      "has_spouse                0.634      0.311      0.525        594       5970\n",
      "place_of_death            0.471      0.201      0.371        159       5535\n",
      "profession                0.632      0.271      0.499        247       5623\n",
      "genre                     0.588      0.118      0.327        170       5546\n",
      "capital                   0.478      0.116      0.294         95       5471\n",
      "place_of_birth            0.612      0.258      0.480        233       5609\n",
      "contains                  0.743      0.298      0.572       3904       9280\n",
      "film_performance          0.662      0.317      0.544        766       6142\n",
      "adjoins                   0.749      0.465      0.667        340       5716\n",
      "is_a                      0.557      0.225      0.430        497       5873\n",
      "has_sibling               0.662      0.315      0.543        499       5875\n",
      "founders                  0.540      0.197      0.401        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.609      0.265      0.477       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=True, use_right=True)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.594      0.252      0.467        301       5677\n",
      "author                    0.720      0.354      0.596        509       5885\n",
      "worked_at                 0.585      0.256      0.465        242       5618\n",
      "parents                   0.682      0.282      0.531        312       5688\n",
      "has_spouse                0.688      0.301      0.548        594       5970\n",
      "place_of_death            0.536      0.233      0.425        159       5535\n",
      "profession                0.635      0.219      0.460        247       5623\n",
      "genre                     0.621      0.106      0.315        170       5546\n",
      "capital                   0.600      0.126      0.343         95       5471\n",
      "place_of_birth            0.574      0.232      0.443        233       5609\n",
      "contains                  0.740      0.302      0.574       3904       9280\n",
      "film_performance          0.694      0.305      0.553        766       6142\n",
      "adjoins                   0.775      0.456      0.680        340       5716\n",
      "is_a                      0.646      0.209      0.456        497       5873\n",
      "has_sibling               0.705      0.273      0.535        499       5875\n",
      "founders                  0.600      0.197      0.426        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.650      0.256      0.489       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=True, use_right=False)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.634      0.196      0.438        301       5677\n",
      "author                    0.781      0.358      0.632        509       5885\n",
      "worked_at                 0.526      0.211      0.405        242       5618\n",
      "parents                   0.653      0.260      0.501        312       5688\n",
      "has_spouse                0.713      0.293      0.554        594       5970\n",
      "place_of_death            0.525      0.132      0.329        159       5535\n",
      "profession                0.759      0.243      0.533        247       5623\n",
      "genre                     0.579      0.129      0.342        170       5546\n",
      "capital                   0.458      0.116      0.288         95       5471\n",
      "place_of_birth            0.683      0.240      0.499        233       5609\n",
      "contains                  0.720      0.285      0.552       3904       9280\n",
      "film_performance          0.683      0.304      0.547        766       6142\n",
      "adjoins                   0.806      0.441      0.692        340       5716\n",
      "is_a                      0.615      0.209      0.443        497       5873\n",
      "has_sibling               0.680      0.230      0.489        499       5875\n",
      "founders                  0.485      0.132      0.316        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.644      0.236      0.472       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=False, use_right=True)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "founders                  0.586      0.216      0.436        380       5756\n",
      "has_spouse                0.736      0.320      0.584        594       5970\n",
      "place_of_birth            0.553      0.245      0.442        233       5609\n",
      "adjoins                   0.823      0.465      0.713        340       5716\n",
      "genre                     0.562      0.106      0.302        170       5546\n",
      "place_of_death            0.547      0.258      0.447        159       5535\n",
      "parents                   0.688      0.276      0.530        312       5688\n",
      "nationality               0.576      0.252      0.458        301       5677\n",
      "is_a                      0.652      0.207      0.456        497       5873\n",
      "worked_at                 0.655      0.298      0.528        242       5618\n",
      "profession                0.753      0.235      0.523        247       5623\n",
      "capital                   0.625      0.158      0.393         95       5471\n",
      "film_performance          0.675      0.307      0.544        766       6142\n",
      "author                    0.681      0.344      0.569        509       5885\n",
      "has_sibling               0.670      0.269      0.516        499       5875\n",
      "contains                  0.780      0.293      0.586       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.660      0.265      0.502       9248      95264\n"
     ]
    }
   ],
   "source": [
    "pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(bigram_pos_tag_featurizer, use_left=True, use_right=False, use_bt_pos=True)],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "There are many options, and this could easily grow into a project. Here are a few ideas:\n",
    "\n",
    "- Try out different classifier models, from `sklearn` and elsewhere.\n",
    "- Add a feature that indicates the length of the middle.\n",
    "- Augment the bag-of-words representation to include bigrams or trigrams (not just unigrams).\n",
    "- Introduce features based on the entity mentions themselves. <!-- \\[SPOILER: it helps a lot, maybe 4% in F-score. And combines nicely with the directional features.\\] -->\n",
    "- Experiment with features based on the context outside (rather than between) the two entity mentions — that is, the words before the first mention, or after the second.\n",
    "- Try adding features which capture syntactic information, such as the dependency-path features used by Mintz et al. 2009. The [NLTK](https://www.nltk.org/) toolkit contains a variety of [parsing algorithms](http://www.nltk.org/api/nltk.parse.html) that may help.\n",
    "- The bag-of-words representation does not permit generalization across word categories such as names of people, places, or companies. Can we do better using word embeddings such as [GloVe](https://nlp.stanford.edu/projects/glove/)?\n",
    "- Consider adding features based on WordNet synsets. Here's a little code to get you started with that:\n",
    "  ```\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  dog_compatible_synsets = wn.synsets('dog', pos='n')\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "simple_bag_of_words_middle_featurizer = partial(simple_bag_of_words_featurizer,use_middle_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "parents                   0.829      0.545      0.751        312       5688\n",
      "has_sibling               0.848      0.234      0.557        499       5875\n",
      "place_of_birth            0.653      0.202      0.451        233       5609\n",
      "author                    0.842      0.534      0.755        509       5885\n",
      "founders                  0.776      0.392      0.649        380       5756\n",
      "genre                     0.596      0.165      0.391        170       5546\n",
      "film_performance          0.793      0.569      0.735        766       6142\n",
      "nationality               0.578      0.196      0.416        301       5677\n",
      "adjoins                   0.821      0.365      0.657        340       5716\n",
      "place_of_death            0.486      0.107      0.284        159       5535\n",
      "contains                  0.799      0.602      0.750       3904       9280\n",
      "worked_at                 0.771      0.264      0.557        242       5618\n",
      "has_spouse                0.917      0.318      0.666        594       5970\n",
      "is_a                      0.659      0.225      0.476        497       5873\n",
      "capital                   0.609      0.147      0.374         95       5471\n",
      "profession                0.580      0.190      0.412        247       5623\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.722      0.316      0.555       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bow_middle_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_middle_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_featurize(words, feature_counter, n, prefix=\"\", directional_prefix=\"\", use_middle_length=False):\n",
    "    for i in range(0, len(words), n):\n",
    "            end = i + n\n",
    "            if (len(words) - i) < n:\n",
    "                end = len(words)\n",
    "            n_gram = ' '.join(words[i:end])\n",
    "            n_gram = directional_prefix + n_gram\n",
    "            feature_counter[prefix + n_gram] += 1\n",
    "    if use_middle_length:\n",
    "        feature_counter[directional_prefix+'NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, \n",
    "                                   directional=False, use_middle_length=False,\n",
    "                                   use_left=False, use_right=False):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        directional_prefix=\"\"\n",
    "        if directional:\n",
    "            directional_prefix = \"FWD_\"\n",
    "        feature_counter = bow_featurize(words, feature_counter, n, \"middle_\", directional_prefix, use_middle_length)\n",
    "        if use_left:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"left_\", directional_prefix, use_middle_length)\n",
    "        if use_right:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"right_\", directional_prefix, use_middle_length)\n",
    "        \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        directional_prefix=\"\"\n",
    "        if directional:\n",
    "            directional_prefix = \"BWD_\"\n",
    "        feature_counter = bow_featurize(words, feature_counter, n, \"middle_\", directional_prefix, use_middle_length)\n",
    "        if use_left:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"left_\", directional_prefix, use_middle_length)\n",
    "        if use_right:\n",
    "            words = ex.middle.split(' ')\n",
    "            feature_counter = bow_featurize(words, feature_counter, n, \"right_\", directional_prefix, use_middle_length)\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_bag_of_words_featurizer = partial(ngrams_bag_of_words_featurizer, n=2)\n",
    "trigrams_bag_of_words_featurizer = partial(ngrams_bag_of_words_featurizer, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.677      0.146      0.392        301       5677\n",
      "author                    0.849      0.409      0.698        509       5885\n",
      "worked_at                 0.700      0.174      0.436        242       5618\n",
      "parents                   0.910      0.388      0.717        312       5688\n",
      "has_spouse                0.915      0.273      0.622        594       5970\n",
      "place_of_death            0.692      0.057      0.213        159       5535\n",
      "profession                0.774      0.166      0.447        247       5623\n",
      "genre                     0.690      0.171      0.429        170       5546\n",
      "capital                   0.500      0.168      0.359         95       5471\n",
      "place_of_birth            0.836      0.197      0.508        233       5609\n",
      "contains                  0.799      0.573      0.740       3904       9280\n",
      "film_performance          0.820      0.483      0.720        766       6142\n",
      "adjoins                   0.886      0.344      0.674        340       5716\n",
      "is_a                      0.758      0.183      0.466        497       5873\n",
      "has_sibling               0.864      0.204      0.525        499       5875\n",
      "founders                  0.824      0.295      0.606        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.781      0.264      0.534       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[bigrams_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.692      0.209      0.474        301       5677\n",
      "author                    0.824      0.580      0.760        509       5885\n",
      "worked_at                 0.736      0.219      0.500        242       5618\n",
      "parents                   0.912      0.433      0.747        312       5688\n",
      "has_spouse                0.910      0.306      0.653        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.781      0.231      0.529        247       5623\n",
      "genre                     0.750      0.282      0.563        170       5546\n",
      "capital                   0.590      0.242      0.458         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.763      0.747      0.759       3904       9280\n",
      "film_performance          0.848      0.555      0.767        766       6142\n",
      "adjoins                   0.848      0.362      0.668        340       5716\n",
      "is_a                      0.758      0.252      0.540        497       5873\n",
      "has_sibling               0.883      0.226      0.559        499       5875\n",
      "founders                  0.844      0.342      0.653        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.791      0.333      0.594       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=True, directional=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.698      0.199      0.465        301       5677\n",
      "author                    0.825      0.574      0.758        509       5885\n",
      "worked_at                 0.718      0.211      0.485        242       5618\n",
      "parents                   0.915      0.417      0.739        312       5688\n",
      "has_spouse                0.914      0.288      0.637        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.759      0.259      0.547        170       5546\n",
      "capital                   0.579      0.232      0.445         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.838      0.627      0.785       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.867      0.365      0.680        340       5716\n",
      "is_a                      0.764      0.247      0.539        497       5873\n",
      "has_sibling               0.897      0.226      0.563        499       5875\n",
      "founders                  0.855      0.326      0.646        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.799      0.317      0.590       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=False, directional=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.698      0.199      0.465        301       5677\n",
      "author                    0.825      0.574      0.758        509       5885\n",
      "worked_at                 0.718      0.211      0.485        242       5618\n",
      "parents                   0.915      0.417      0.739        312       5688\n",
      "has_spouse                0.914      0.288      0.637        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.759      0.259      0.547        170       5546\n",
      "capital                   0.579      0.232      0.445         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.838      0.627      0.785       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.867      0.365      0.680        340       5716\n",
      "is_a                      0.764      0.247      0.539        497       5873\n",
      "has_sibling               0.897      0.226      0.563        499       5875\n",
      "founders                  0.855      0.326      0.646        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.799      0.317      0.590       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=False, use_right=True, directional=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.667      0.173      0.424        301       5677\n",
      "author                    0.750      0.519      0.689        509       5885\n",
      "worked_at                 0.761      0.211      0.500        242       5618\n",
      "parents                   0.868      0.420      0.715        312       5688\n",
      "has_spouse                0.909      0.301      0.648        594       5970\n",
      "place_of_death            0.412      0.044      0.154        159       5535\n",
      "profession                0.729      0.206      0.484        247       5623\n",
      "genre                     0.685      0.218      0.479        170       5546\n",
      "capital                   0.528      0.200      0.397         95       5471\n",
      "place_of_birth            0.806      0.215      0.520        233       5609\n",
      "contains                  0.786      0.587      0.736       3904       9280\n",
      "film_performance          0.814      0.516      0.730        766       6142\n",
      "adjoins                   0.847      0.341      0.653        340       5716\n",
      "is_a                      0.729      0.227      0.506        497       5873\n",
      "has_sibling               0.846      0.208      0.525        499       5875\n",
      "founders                  0.802      0.342      0.632        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.746      0.295      0.549       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.662      0.163      0.410        301       5677\n",
      "author                    0.759      0.513      0.692        509       5885\n",
      "worked_at                 0.731      0.202      0.480        242       5618\n",
      "parents                   0.882      0.407      0.715        312       5688\n",
      "has_spouse                0.911      0.291      0.639        594       5970\n",
      "place_of_death            0.500      0.050      0.179        159       5535\n",
      "profession                0.750      0.194      0.477        247       5623\n",
      "genre                     0.706      0.212      0.481        170       5546\n",
      "capital                   0.529      0.189      0.390         95       5471\n",
      "place_of_birth            0.817      0.210      0.518        233       5609\n",
      "contains                  0.791      0.582      0.738       3904       9280\n",
      "film_performance          0.820      0.512      0.732        766       6142\n",
      "adjoins                   0.854      0.344      0.659        340       5716\n",
      "is_a                      0.729      0.211      0.489        497       5873\n",
      "has_sibling               0.850      0.204      0.521        499       5875\n",
      "founders                  0.807      0.318      0.617        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.756      0.288      0.546       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=True, use_right=False)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.662      0.163      0.410        301       5677\n",
      "author                    0.759      0.513      0.692        509       5885\n",
      "worked_at                 0.731      0.202      0.480        242       5618\n",
      "parents                   0.882      0.407      0.715        312       5688\n",
      "has_spouse                0.911      0.291      0.639        594       5970\n",
      "place_of_death            0.500      0.050      0.179        159       5535\n",
      "profession                0.750      0.194      0.477        247       5623\n",
      "genre                     0.706      0.212      0.481        170       5546\n",
      "capital                   0.529      0.189      0.390         95       5471\n",
      "place_of_birth            0.817      0.210      0.518        233       5609\n",
      "contains                  0.791      0.582      0.738       3904       9280\n",
      "film_performance          0.820      0.512      0.732        766       6142\n",
      "adjoins                   0.854      0.344      0.659        340       5716\n",
      "is_a                      0.729      0.211      0.489        497       5873\n",
      "has_sibling               0.850      0.204      0.521        499       5875\n",
      "founders                  0.807      0.318      0.617        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.756      0.288      0.546       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, use_left=False, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.732      0.136      0.390        301       5677\n",
      "author                    0.777      0.458      0.682        509       5885\n",
      "worked_at                 0.811      0.124      0.385        242       5618\n",
      "parents                   0.925      0.276      0.629        312       5688\n",
      "has_spouse                0.940      0.239      0.593        594       5970\n",
      "place_of_death            0.714      0.031      0.134        159       5535\n",
      "profession                0.742      0.093      0.310        247       5623\n",
      "genre                     0.857      0.141      0.426        170       5546\n",
      "capital                   0.423      0.116      0.276         95       5471\n",
      "place_of_birth            0.800      0.103      0.340        233       5609\n",
      "contains                  0.808      0.478      0.710       3904       9280\n",
      "film_performance          0.789      0.381      0.650        766       6142\n",
      "adjoins                   0.884      0.315      0.649        340       5716\n",
      "is_a                      0.831      0.109      0.357        497       5873\n",
      "has_sibling               0.859      0.134      0.413        499       5875\n",
      "founders                  0.806      0.208      0.512        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.794      0.209      0.466       9248      95264\n"
     ]
    }
   ],
   "source": [
    "trigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[trigrams_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_bag_of_words_featurizer_use_middle = partial(ngrams_bag_of_words_featurizer, n=2, use_middle_length=True)\n",
    "bigrams_bag_of_words_featurizer_use_mid_direction = partial(ngrams_bag_of_words_featurizer, n=2, directional=True, use_middle_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.818      0.470      0.713        766       6142\n",
      "place_of_birth            0.692      0.193      0.456        233       5609\n",
      "nationality               0.592      0.140      0.359        301       5677\n",
      "place_of_death            0.467      0.044      0.160        159       5535\n",
      "has_spouse                0.874      0.256      0.589        594       5970\n",
      "genre                     0.738      0.182      0.459        170       5546\n",
      "founders                  0.766      0.276      0.566        380       5756\n",
      "has_sibling               0.890      0.178      0.495        499       5875\n",
      "is_a                      0.731      0.175      0.447        497       5873\n",
      "profession                0.684      0.158      0.411        247       5623\n",
      "worked_at                 0.682      0.186      0.445        242       5618\n",
      "contains                  0.808      0.548      0.738       3904       9280\n",
      "author                    0.777      0.458      0.682        509       5885\n",
      "capital                   0.750      0.158      0.429         95       5471\n",
      "adjoins                   0.925      0.326      0.677        340       5716\n",
      "parents                   0.857      0.385      0.688        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.753      0.258      0.519       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[bigrams_bag_of_words_featurizer_use_middle],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.692      0.209      0.474        301       5677\n",
      "author                    0.824      0.580      0.760        509       5885\n",
      "worked_at                 0.736      0.219      0.500        242       5618\n",
      "parents                   0.912      0.433      0.747        312       5688\n",
      "has_spouse                0.910      0.306      0.653        594       5970\n",
      "place_of_death            0.692      0.113      0.342        159       5535\n",
      "profession                0.781      0.231      0.529        247       5623\n",
      "genre                     0.750      0.282      0.563        170       5546\n",
      "capital                   0.590      0.242      0.458         95       5471\n",
      "place_of_birth            0.825      0.223      0.536        233       5609\n",
      "contains                  0.763      0.747      0.759       3904       9280\n",
      "film_performance          0.848      0.555      0.767        766       6142\n",
      "adjoins                   0.848      0.362      0.668        340       5716\n",
      "is_a                      0.758      0.252      0.540        497       5873\n",
      "has_sibling               0.883      0.226      0.559        499       5875\n",
      "founders                  0.844      0.342      0.653        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.791      0.333      0.594       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                        use_left=True, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.713      0.223      0.495        301       5677\n",
      "author                    0.853      0.572      0.777        509       5885\n",
      "worked_at                 0.803      0.202      0.504        242       5618\n",
      "parents                   0.915      0.413      0.736        312       5688\n",
      "has_spouse                0.877      0.288      0.622        594       5970\n",
      "place_of_death            0.609      0.088      0.279        159       5535\n",
      "profession                0.800      0.227      0.531        247       5623\n",
      "genre                     0.754      0.288      0.570        170       5546\n",
      "capital                   0.618      0.221      0.455         95       5471\n",
      "place_of_birth            0.864      0.219      0.544        233       5609\n",
      "contains                  0.786      0.688      0.764       3904       9280\n",
      "film_performance          0.850      0.554      0.768        766       6142\n",
      "adjoins                   0.863      0.353      0.670        340       5716\n",
      "is_a                      0.800      0.233      0.539        497       5873\n",
      "has_sibling               0.903      0.204      0.536        499       5875\n",
      "founders                  0.871      0.337      0.661        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.805      0.319      0.591       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                         use_middle_length=True, use_left=True, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.707      0.176      0.441        301       5677\n",
      "author                    0.858      0.558      0.775        509       5885\n",
      "worked_at                 0.754      0.190      0.473        242       5618\n",
      "parents                   0.922      0.417      0.742        312       5688\n",
      "has_spouse                0.881      0.286      0.622        594       5970\n",
      "place_of_death            0.619      0.082      0.267        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.784      0.235      0.535        170       5546\n",
      "capital                   0.576      0.200      0.419         95       5471\n",
      "place_of_birth            0.864      0.219      0.544        233       5609\n",
      "contains                  0.786      0.688      0.764       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.875      0.350      0.673        340       5716\n",
      "is_a                      0.803      0.221      0.526        497       5873\n",
      "has_sibling               0.904      0.206      0.539        499       5875\n",
      "founders                  0.883      0.318      0.652        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.803      0.307      0.578       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                         use_middle_length=True, use_left=False, use_right=True)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.707      0.176      0.441        301       5677\n",
      "author                    0.858      0.558      0.775        509       5885\n",
      "worked_at                 0.754      0.190      0.473        242       5618\n",
      "parents                   0.922      0.417      0.742        312       5688\n",
      "has_spouse                0.881      0.286      0.622        594       5970\n",
      "place_of_death            0.619      0.082      0.267        159       5535\n",
      "profession                0.779      0.215      0.511        247       5623\n",
      "genre                     0.784      0.235      0.535        170       5546\n",
      "capital                   0.576      0.200      0.419         95       5471\n",
      "place_of_birth            0.864      0.219      0.544        233       5609\n",
      "contains                  0.786      0.688      0.764       3904       9280\n",
      "film_performance          0.852      0.548      0.767        766       6142\n",
      "adjoins                   0.875      0.350      0.673        340       5716\n",
      "is_a                      0.803      0.221      0.526        497       5873\n",
      "has_sibling               0.904      0.206      0.539        499       5875\n",
      "founders                  0.883      0.318      0.652        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.803      0.307      0.578       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[partial(ngrams_bag_of_words_featurizer, n=2, directional=True, \n",
    "                         use_middle_length=True, use_left=True, use_right=False)],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.827      0.551      0.752        766       6142\n",
      "place_of_birth            0.724      0.236      0.512        233       5609\n",
      "nationality               0.678      0.266      0.517        301       5677\n",
      "place_of_death            0.559      0.119      0.322        159       5535\n",
      "has_spouse                0.931      0.295      0.650        594       5970\n",
      "genre                     0.829      0.371      0.665        170       5546\n",
      "founders                  0.794      0.376      0.650        380       5756\n",
      "has_sibling               0.930      0.240      0.591        499       5875\n",
      "is_a                      0.812      0.348      0.641        497       5873\n",
      "profession                0.800      0.340      0.630        247       5623\n",
      "worked_at                 0.744      0.277      0.556        242       5618\n",
      "contains                  0.783      0.379      0.645       3904       9280\n",
      "author                    0.794      0.538      0.725        509       5885\n",
      "capital                   0.714      0.158      0.419         95       5471\n",
      "adjoins                   0.884      0.426      0.728        340       5716\n",
      "parents                   0.859      0.545      0.770        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.791      0.342      0.611       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_entities_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True)\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_entities_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.769      0.453      0.675        766       6142\n",
      "place_of_birth            0.435      0.086      0.240        233       5609\n",
      "nationality               0.451      0.136      0.308        301       5677\n",
      "place_of_death            0.130      0.019      0.060        159       5535\n",
      "has_spouse                0.653      0.133      0.366        594       5970\n",
      "genre                     0.706      0.212      0.481        170       5546\n",
      "founders                  0.522      0.092      0.270        380       5756\n",
      "has_sibling               0.783      0.253      0.551        499       5875\n",
      "is_a                      0.642      0.282      0.511        497       5873\n",
      "profession                0.800      0.211      0.513        247       5623\n",
      "worked_at                 0.659      0.120      0.347        242       5618\n",
      "contains                  0.703      0.440      0.628       3904       9280\n",
      "author                    0.689      0.330      0.566        509       5885\n",
      "capital                   0.515      0.179      0.374         95       5471\n",
      "adjoins                   0.867      0.403      0.705        340       5716\n",
      "parents                   0.561      0.103      0.296        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.618      0.216      0.431       9248      95264\n"
     ]
    }
   ],
   "source": [
    "left_bag_of_words_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True, context_section='left')\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[left_bag_of_words_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.729      0.386      0.619        766       6142\n",
      "place_of_birth            0.326      0.060      0.173        233       5609\n",
      "nationality               0.400      0.100      0.250        301       5677\n",
      "place_of_death            0.421      0.050      0.170        159       5535\n",
      "has_spouse                0.537      0.121      0.319        594       5970\n",
      "genre                     0.750      0.212      0.497        170       5546\n",
      "founders                  0.517      0.082      0.250        380       5756\n",
      "has_sibling               0.676      0.146      0.392        499       5875\n",
      "is_a                      0.619      0.219      0.454        497       5873\n",
      "profession                0.765      0.158      0.432        247       5623\n",
      "worked_at                 0.526      0.083      0.254        242       5618\n",
      "contains                  0.682      0.419      0.606       3904       9280\n",
      "author                    0.632      0.240      0.476        509       5885\n",
      "capital                   0.465      0.211      0.375         95       5471\n",
      "adjoins                   0.914      0.409      0.733        340       5716\n",
      "parents                   0.578      0.119      0.326        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.596      0.188      0.395       9248      95264\n"
     ]
    }
   ],
   "source": [
    "right_bag_of_words_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True, context_section='right')\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[right_bag_of_words_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.751      0.617      0.720        766       6142\n",
      "place_of_birth            0.562      0.253      0.452        233       5609\n",
      "nationality               0.521      0.326      0.465        301       5677\n",
      "place_of_death            0.383      0.145      0.288        159       5535\n",
      "has_spouse                0.872      0.332      0.658        594       5970\n",
      "genre                     0.764      0.553      0.710        170       5546\n",
      "founders                  0.803      0.418      0.678        380       5756\n",
      "has_sibling               0.884      0.305      0.640        499       5875\n",
      "is_a                      0.742      0.493      0.674        497       5873\n",
      "profession                0.756      0.478      0.677        247       5623\n",
      "worked_at                 0.692      0.343      0.575        242       5618\n",
      "contains                  0.778      0.426      0.668       3904       9280\n",
      "author                    0.788      0.585      0.737        509       5885\n",
      "capital                   0.553      0.221      0.425         95       5471\n",
      "adjoins                   0.817      0.474      0.714        340       5716\n",
      "parents                   0.832      0.571      0.762        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.719      0.409      0.615       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_middle_entities_featurizer = partial(simple_bag_of_words_featurizer, \n",
    "                                                         use_entities=True, use_middle_length=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_middle_entities_featurizer],\n",
    "    model_factory=svc_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer2(kbt, corpus, feature_counter, \n",
    "                                    use_middle_length=False, \n",
    "                                    use_entities=False,\n",
    "                                    context_section='middle', # can be 'left', 'right', or 'middle'\n",
    "                                    use_synsets=False):\n",
    "    synset_prefix = \"synset_:\"\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "        \n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos_pair in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[word] += 1\n",
    "                        for hyponym in syn.hyponyms():\n",
    "                            feature_counter[synset_prefix+hyponym.name()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        \n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "\n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos_pair in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[word] += 1\n",
    "                        for hyponym in syn.hyponyms():\n",
    "                            feature_counter[synset_prefix+hyponym.name()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.698      0.615      0.679        766       6142\n",
      "place_of_birth            0.460      0.223      0.380        233       5609\n",
      "nationality               0.374      0.243      0.338        301       5677\n",
      "place_of_death            0.329      0.145      0.262        159       5535\n",
      "has_spouse                0.795      0.327      0.618        594       5970\n",
      "genre                     0.400      0.306      0.377        170       5546\n",
      "founders                  0.630      0.447      0.582        380       5756\n",
      "has_sibling               0.730      0.261      0.537        499       5875\n",
      "is_a                      0.496      0.262      0.421        497       5873\n",
      "profession                0.458      0.219      0.376        247       5623\n",
      "worked_at                 0.565      0.306      0.483        242       5618\n",
      "contains                  0.755      0.593      0.716       3904       9280\n",
      "author                    0.704      0.603      0.681        509       5885\n",
      "capital                   0.556      0.263      0.455         95       5471\n",
      "adjoins                   0.649      0.326      0.542        340       5716\n",
      "parents                   0.776      0.545      0.715        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.586      0.355      0.510       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_synsets_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                                         use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_synsets_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.738      0.598      0.705        766       6142\n",
      "place_of_birth            0.481      0.219      0.388        233       5609\n",
      "nationality               0.484      0.306      0.434        301       5677\n",
      "place_of_death            0.403      0.157      0.307        159       5535\n",
      "has_spouse                0.877      0.300      0.633        594       5970\n",
      "genre                     0.574      0.412      0.532        170       5546\n",
      "founders                  0.647      0.400      0.576        380       5756\n",
      "has_sibling               0.772      0.244      0.539        499       5875\n",
      "is_a                      0.676      0.356      0.573        497       5873\n",
      "profession                0.730      0.328      0.586        247       5623\n",
      "worked_at                 0.615      0.277      0.494        242       5618\n",
      "contains                  0.768      0.472      0.682       3904       9280\n",
      "author                    0.767      0.587      0.723        509       5885\n",
      "capital                   0.629      0.232      0.468         95       5471\n",
      "adjoins                   0.671      0.324      0.552        340       5716\n",
      "parents                   0.779      0.519      0.708        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.663      0.358      0.556       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_all_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                            use_entities=True, use_middle_length=True, use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_all_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.738      0.598      0.705        766       6142\n",
      "place_of_birth            0.481      0.219      0.388        233       5609\n",
      "nationality               0.484      0.306      0.434        301       5677\n",
      "place_of_death            0.403      0.157      0.307        159       5535\n",
      "has_spouse                0.877      0.300      0.633        594       5970\n",
      "genre                     0.574      0.412      0.532        170       5546\n",
      "founders                  0.647      0.400      0.576        380       5756\n",
      "has_sibling               0.772      0.244      0.539        499       5875\n",
      "is_a                      0.676      0.356      0.573        497       5873\n",
      "profession                0.730      0.328      0.586        247       5623\n",
      "worked_at                 0.615      0.277      0.494        242       5618\n",
      "contains                  0.768      0.472      0.682       3904       9280\n",
      "author                    0.767      0.587      0.723        509       5885\n",
      "capital                   0.629      0.232      0.468         95       5471\n",
      "adjoins                   0.671      0.324      0.552        340       5716\n",
      "parents                   0.779      0.519      0.708        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.663      0.358      0.556       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_all_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                            use_entities=True, use_middle_length=True, use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_all_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.847      0.645      0.797        766       6142\n",
      "place_of_birth            0.692      0.232      0.495        233       5609\n",
      "nationality               0.623      0.219      0.455        301       5677\n",
      "place_of_death            0.550      0.138      0.345        159       5535\n",
      "has_spouse                0.896      0.347      0.680        594       5970\n",
      "genre                     0.657      0.259      0.502        170       5546\n",
      "founders                  0.823      0.416      0.688        380       5756\n",
      "has_sibling               0.897      0.244      0.585        499       5875\n",
      "is_a                      0.747      0.225      0.510        497       5873\n",
      "profession                0.740      0.231      0.514        247       5623\n",
      "worked_at                 0.727      0.264      0.539        242       5618\n",
      "contains                  0.845      0.652      0.798       3904       9280\n",
      "author                    0.838      0.589      0.773        509       5885\n",
      "capital                   0.677      0.221      0.479         95       5471\n",
      "adjoins                   0.890      0.403      0.717        340       5716\n",
      "parents                   0.848      0.519      0.753        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.769      0.350      0.602       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_middle_featurizer = partial(directional_bag_of_words_featurizer, use_middle_length=True)\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_middle_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.855      0.633      0.799        766       6142\n",
      "place_of_birth            0.772      0.262      0.556        233       5609\n",
      "nationality               0.705      0.326      0.572        301       5677\n",
      "place_of_death            0.565      0.164      0.379        159       5535\n",
      "has_spouse                0.944      0.311      0.671        594       5970\n",
      "genre                     0.873      0.406      0.710        170       5546\n",
      "founders                  0.828      0.379      0.669        380       5756\n",
      "has_sibling               0.947      0.251      0.609        499       5875\n",
      "is_a                      0.811      0.372      0.656        497       5873\n",
      "profession                0.828      0.389      0.675        247       5623\n",
      "worked_at                 0.756      0.281      0.565        242       5618\n",
      "contains                  0.848      0.414      0.701       3904       9280\n",
      "author                    0.856      0.621      0.796        509       5885\n",
      "capital                   0.697      0.242      0.507         95       5471\n",
      "adjoins                   0.936      0.385      0.728        340       5716\n",
      "parents                   0.859      0.510      0.756        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.818      0.372      0.647       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_middle_entities_featurizer = partial(directional_bag_of_words_featurizer, \n",
    "                                                 use_middle_length=True, use_entities=True)\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_middle_entities_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True) \n",
    "    return bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.877      0.664      0.824        301       5677\n",
      "author                    0.878      0.723      0.842        509       5885\n",
      "worked_at                 0.777      0.302      0.591        242       5618\n",
      "parents                   0.881      0.667      0.828        312       5688\n",
      "has_spouse                0.822      0.646      0.780        594       5970\n",
      "place_of_death            0.779      0.421      0.666        159       5535\n",
      "profession                0.917      0.534      0.802        247       5623\n",
      "genre                     0.949      0.329      0.690        170       5546\n",
      "capital                   0.462      0.126      0.302         95       5471\n",
      "place_of_birth            0.841      0.498      0.739        233       5609\n",
      "contains                  0.865      0.690      0.823       3904       9280\n",
      "film_performance          0.858      0.696      0.820        766       6142\n",
      "adjoins                   0.817      0.315      0.619        340       5716\n",
      "is_a                      0.869      0.559      0.782        497       5873\n",
      "has_sibling               0.853      0.651      0.803        499       5875\n",
      "founders                  0.732      0.295      0.565        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.824      0.507      0.717       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_ngrams_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True) \n",
    "    return bigrams_bag_of_words_featurizer(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "nationality               0.898      0.645      0.833        301       5677\n",
      "author                    0.872      0.721      0.837        509       5885\n",
      "worked_at                 0.813      0.252      0.563        242       5618\n",
      "parents                   0.889      0.667      0.833        312       5688\n",
      "has_spouse                0.846      0.603      0.783        594       5970\n",
      "place_of_death            0.847      0.384      0.682        159       5535\n",
      "profession                0.923      0.486      0.782        247       5623\n",
      "genre                     0.925      0.288      0.641        170       5546\n",
      "capital                   0.480      0.126      0.308         95       5471\n",
      "place_of_birth            0.895      0.438      0.740        233       5609\n",
      "contains                  0.870      0.674      0.822       3904       9280\n",
      "film_performance          0.870      0.688      0.826        766       6142\n",
      "adjoins                   0.836      0.285      0.603        340       5716\n",
      "is_a                      0.881      0.549      0.786        497       5873\n",
      "has_sibling               0.870      0.629      0.808        499       5875\n",
      "founders                  0.742      0.311      0.581        380       5756\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.841      0.484      0.714       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_ngrams_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_ngrams_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_featurizer2(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    return bigrams_bag_of_words_featurizer(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "film_performance          0.858      0.692      0.818        766       6142\n",
      "place_of_birth            0.779      0.468      0.687        233       5609\n",
      "nationality               0.871      0.648      0.815        301       5677\n",
      "place_of_death            0.739      0.409      0.636        159       5535\n",
      "has_spouse                0.836      0.616      0.780        594       5970\n",
      "genre                     0.908      0.347      0.686        170       5546\n",
      "founders                  0.825      0.458      0.711        380       5756\n",
      "has_sibling               0.909      0.637      0.837        499       5875\n",
      "is_a                      0.861      0.547      0.772        497       5873\n",
      "profession                0.899      0.543      0.795        247       5623\n",
      "worked_at                 0.758      0.285      0.569        242       5618\n",
      "contains                  0.857      0.689      0.817       3904       9280\n",
      "author                    0.857      0.750      0.833        509       5885\n",
      "capital                   0.636      0.147      0.383         95       5471\n",
      "adjoins                   0.797      0.335      0.625        340       5716\n",
      "parents                   0.867      0.692      0.826        312       5688\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.828      0.516      0.724       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_featurizer2],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_direct_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    return bigrams_bag_of_words_featurizer_use_mid_direction(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_bow_pos_ngrams_direct_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_direct_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter)\n",
    "    return bigrams_bag_of_words_featurizer(kbt, corpus, feature_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "founders                  0.794      0.476      0.700        380       5756\n",
      "has_spouse                0.814      0.618      0.765        594       5970\n",
      "place_of_birth            0.766      0.464      0.678        233       5609\n",
      "adjoins                   0.839      0.306      0.622        340       5716\n",
      "genre                     0.871      0.359      0.678        170       5546\n",
      "place_of_death            0.723      0.428      0.636        159       5535\n",
      "parents                   0.886      0.670      0.832        312       5688\n",
      "nationality               0.848      0.668      0.805        301       5677\n",
      "is_a                      0.903      0.561      0.805        497       5873\n",
      "worked_at                 0.870      0.277      0.609        242       5618\n",
      "profession                0.944      0.547      0.824        247       5623\n",
      "capital                   0.609      0.147      0.374         95       5471\n",
      "film_performance          0.853      0.698      0.817        766       6142\n",
      "author                    0.873      0.741      0.843        509       5885\n",
      "has_sibling               0.871      0.651      0.816        499       5875\n",
      "contains                  0.863      0.690      0.822       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.833      0.519      0.727       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_ngrams_final(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True, use_bt_pos=True)\n",
    "    return ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, directional=True, \n",
    "                                        use_left=True, use_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "parents                   0.879      0.673      0.828        312       5688\n",
      "has_sibling               0.870      0.681      0.824        499       5875\n",
      "place_of_birth            0.764      0.485      0.685        233       5609\n",
      "author                    0.884      0.762      0.857        509       5885\n",
      "founders                  0.811      0.553      0.742        380       5756\n",
      "genre                     0.875      0.329      0.657        170       5546\n",
      "film_performance          0.891      0.749      0.859        766       6142\n",
      "nationality               0.817      0.698      0.790        301       5677\n",
      "adjoins                   0.753      0.512      0.688        340       5716\n",
      "place_of_death            0.716      0.491      0.655        159       5535\n",
      "contains                  0.876      0.769      0.852       3904       9280\n",
      "worked_at                 0.763      0.438      0.664        242       5618\n",
      "has_spouse                0.837      0.684      0.801        594       5970\n",
      "is_a                      0.824      0.547      0.748        497       5873\n",
      "capital                   0.610      0.263      0.483         95       5471\n",
      "profession                0.903      0.530      0.792        247       5623\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.817      0.573      0.745       9248      95264\n"
     ]
    }
   ],
   "source": [
    "ensembled_bow_pos_ngrams_direct_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_ngrams_final],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_bag_of_words_featurizer(kbt, corpus, feature_counter, glove_lookup,\n",
    "                                context_section='middle',\n",
    "                                use_middle_length=False,\n",
    "                                glove_dims=300): # can be 'left', 'right', or 'middle'\n",
    "    fwd_glove_vector = np.zeros(glove_dims)\n",
    "    bwd_glove_vector = np.zeros(glove_dims)\n",
    "\n",
    "    sbj_glove = glove_lookup.get(kbt.sbj, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))\n",
    "    obj_glove = glove_lookup.get(kbt.obj, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))\n",
    "\n",
    "    feature_prefix = \"sbj_glove:\"\n",
    "    for i, feature in enumerate(sbj_glove):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "        \n",
    "    feature_prefix = \"obj_glove:\"\n",
    "    for i, feature in enumerate(obj_glove):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            #words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "            words = ' '.join((ex.mention_1, ex.middle, ex.mention_2)).split(' ')\n",
    "        for word in words:\n",
    "            fwd_glove_vector += glove_lookup.get(word, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))/len(words)\n",
    "        if use_middle_length:\n",
    "            feature_counter['FWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            #words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "            words = ' '.join((ex.mention_1, ex.middle, ex.mention_2)).split(' ')\n",
    "        for word in words:\n",
    "            bwd_glove_vector += glove_lookup.get(word, np.array([random.uniform(-0.5, 0.5) for i in range(glove_dims)]))/len(words)\n",
    "        if use_middle_length:\n",
    "            feature_counter['BWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "    \n",
    "    feature_prefix = \"fwd_glove_:\"\n",
    "    for i, feature in enumerate(fwd_glove_vector):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "\n",
    "    feature_prefix = \"bwd_glove_:\"\n",
    "    for i, feature in enumerate(bwd_glove_vector):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "    \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"glove_featurizer = partial(glove_bag_of_words_featurizer, context_section='all', glove_lookup=glove_lookup)\\n\\nglove_results = rel_ext.experiment(\\n    splits,\\n    train_split='train',\\n    test_split='dev',\\n    featurizers=[glove_featurizer],\\n    model_factory=model_factory_2k,\\n    verbose=True)\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''glove_featurizer = partial(glove_bag_of_words_featurizer, context_section='all', glove_lookup=glove_lookup)\n",
    "\n",
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[glove_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_length_featurizer = partial(glove_bag_of_words_featurizer, \n",
    "                                  context_section='all', \n",
    "                                  use_middle_length=True, glove_lookup=glove_lookup)\n",
    "\n",
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[glove_length_featurizer],\n",
    "    model_factory=model_factory_2k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembled_bow_pos_glove_featurizer(kbt, corpus, feature_counter):\n",
    "    feature_counter = directional_bag_of_words_featurizer(kbt, corpus, feature_counter, use_middle_length=True,\n",
    "                                        use_entities=True, include_left=True, include_right=True)\n",
    "    feature_counter = bigram_pos_tag_featurizer(kbt, corpus, feature_counter, use_left=True)\n",
    "    feature_counter =  ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, directional=True, \n",
    "                                        use_left=True, use_right=True)\n",
    "    return glove_bag_of_words_featurizer(kbt, corpus, feature_counter, \n",
    "                                         context_section='all', glove_lookup=glove_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[ensembled_bow_pos_glove_featurizer],\n",
    "    model_factory=model_factory_4k,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('notebook_env.db')\n",
    "#dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release a test set right after class on April 29. The announcement will go out on Piazza. You will evaluate your custom model from the previous question on these new datasets using the function `rel_ext.bake_off_experiment`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187248\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on May 1. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "parents                   0.909      0.705      0.860        427       7111\n",
      "has_sibling               0.885      0.709      0.843        717       7401\n",
      "place_of_birth            0.820      0.533      0.740        291       6975\n",
      "author                    0.891      0.726      0.852        645       7329\n",
      "founders                  0.758      0.565      0.710        444       7128\n",
      "genre                     0.871      0.324      0.652        188       6872\n",
      "film_performance          0.871      0.743      0.842       1011       7695\n",
      "nationality               0.841      0.593      0.776        383       7067\n",
      "adjoins                   0.829      0.587      0.766        438       7122\n",
      "place_of_death            0.775      0.465      0.684        200       6884\n",
      "contains                  0.862      0.774      0.843       3808      10492\n",
      "worked_at                 0.848      0.520      0.753        323       7007\n",
      "has_spouse                0.836      0.672      0.797        780       7464\n",
      "is_a                      0.830      0.496      0.732        611       7295\n",
      "capital                   0.689      0.270      0.525        115       6799\n",
      "profession                0.849      0.416      0.703        310       6994\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.835      0.569      0.755      10691     117635\n"
     ]
    }
   ],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n",
    "rel_ext.bake_off_experiment(ensembled_bow_pos_ngrams_direct_results,\n",
    "    rel_ext_data_home,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average f-score (an F_0.5 score) as reported \n",
    "# by the code above. Please enter only a number between \n",
    "# 0 and 1 inclusive. Please do not remove this comment.\n",
    "0.755"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
