{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney\"\n",
    "__version__ = \"CS224U, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Baseline](#Baseline)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Different model factory [1 point]](#Different-model-factory-[1-point])\n",
    "  1. [Directional unigram features [2 points]](#Directional-unigram-features-[2-points])\n",
    "  1. [The part-of-speech tags of the \"middle\" words [2 points]](#The-part-of-speech-tags-of-the-\"middle\"-words-[2-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to the developing really effective relation extraction systems using distant supervision. \n",
    "\n",
    "As with the previous assignments, this notebook first establishes a baseline system. The initial homework questions ask you to create additional baselines and suggest areas for innovation, and the final homework question asks you to develop an original system for you to enter into the bake-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import rel_ext\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we unite our corpus and KB into a dataset, and create some splits for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_HOME = '/home/kd/data/data'\n",
    "rel_ext_data_home = os.path.join(DATA_HOME, 'rel_ext_data')\n",
    "GLOVE_HOME = os.path.join(DATA_HOME, 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not wedded to this set-up for splits. The bake-off will be conducted on a previously unseen test-set, so all of the data in `dataset` is fair game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.79, 0.20],\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': Corpus with 331,696 examples; KB with 45,884 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 263,285 examples; KB with 36,191 triples}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter, \n",
    "                                    use_middle_length=False, \n",
    "                                    use_entities=False,\n",
    "                                    context_section='middle', # can be 'left', 'right', or 'middle'\n",
    "                                    use_synsets=False):\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "        \n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            print(\"word[1]=\"+words[0])\n",
    "            print(\"pos_s[1]=\"+pos_s[0])\n",
    "            for word, pos in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word, pos_word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[syn.lemma()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        \n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "\n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for pos_pair in pos_s:\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word, pos_word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[syn.lemma()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "featurizers = [simple_bag_of_words_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')\n",
    "model_factory_400 = lambda: LogisticRegression(fit_intercept=True, solver='liblinear', max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.864      0.374      0.684        340       5716\n",
      "parents                   0.834      0.532      0.749        312       5688\n",
      "founders                  0.835      0.413      0.693        380       5756\n",
      "genre                     0.558      0.171      0.384        170       5546\n",
      "worked_at                 0.739      0.269      0.547        242       5618\n",
      "contains                  0.797      0.605      0.750       3904       9280\n",
      "author                    0.830      0.507      0.736        509       5885\n",
      "nationality               0.675      0.179      0.435        301       5677\n",
      "film_performance          0.783      0.547      0.721        766       6142\n",
      "place_of_birth            0.657      0.197      0.448        233       5609\n",
      "is_a                      0.671      0.225      0.481        497       5873\n",
      "place_of_death            0.500      0.126      0.313        159       5535\n",
      "has_sibling               0.860      0.246      0.574        499       5875\n",
      "capital                   0.595      0.232      0.453         95       5471\n",
      "profession                0.647      0.178      0.424        247       5623\n",
      "has_spouse                0.835      0.333      0.642        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.730      0.321      0.565       9248      95264\n"
     ]
    }
   ],
   "source": [
    "baseline_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying model weights might yield insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     2.542 Taluks\n",
      "     2.473 Córdoba\n",
      "     2.453 Valais\n",
      "     ..... .....\n",
      "    -1.336 America\n",
      "    -1.382 Spain\n",
      "    -2.202 Earth\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     5.260 son\n",
      "     4.418 daughter\n",
      "     4.296 father\n",
      "     ..... .....\n",
      "    -1.355 Tyndareus\n",
      "    -1.597 Tina\n",
      "    -1.843 played\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     4.087 founder\n",
      "     3.728 founded\n",
      "     3.064 co-founder\n",
      "     ..... .....\n",
      "    -1.589 MD\n",
      "    -1.600 novel\n",
      "    -1.926 band\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     2.787 series\n",
      "     2.647 movie\n",
      "     2.456 album\n",
      "     ..... .....\n",
      "    -1.702 starring\n",
      "    -1.783 at\n",
      "    -1.955 original\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     3.077 CEO\n",
      "     2.860 professor\n",
      "     2.672 president\n",
      "     ..... .....\n",
      "    -1.374 India\n",
      "    -1.651 or\n",
      "    -1.746 state\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     2.313 districts\n",
      "     2.174 third-largest\n",
      "     2.146 bordered\n",
      "     ..... .....\n",
      "    -3.074 Lancashire\n",
      "    -3.186 Midlands\n",
      "    -3.551 Ceylon\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     2.405 book\n",
      "     2.311 wrote\n",
      "     2.263 books\n",
      "     ..... .....\n",
      "    -2.133 1925\n",
      "    -2.967 Daisy\n",
      "    -5.863 dystopian\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     2.644 born\n",
      "     1.837 Set\n",
      "     1.823 caliph\n",
      "     ..... .....\n",
      "    -1.529 American\n",
      "    -1.704 U.S.\n",
      "    -1.957 state\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     4.023 co-starring\n",
      "     4.012 starring\n",
      "     3.407 opposite\n",
      "     ..... .....\n",
      "    -2.113 She\n",
      "    -2.228 Anjaani\n",
      "    -2.228 Anjaana\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     3.819 born\n",
      "     2.947 birthplace\n",
      "     2.400 mayor\n",
      "     ..... .....\n",
      "    -1.462 and\n",
      "    -1.815 state\n",
      "    -1.994 Oldham\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     2.787 family\n",
      "     2.646 genus\n",
      "     2.567 \n",
      "     ..... .....\n",
      "    -1.608 on\n",
      "    -1.631 now\n",
      "    -1.704 nightshade\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     2.378 died\n",
      "     1.988 assassinated\n",
      "     1.901 son\n",
      "     ..... .....\n",
      "    -1.130 province\n",
      "    -1.214 and\n",
      "    -1.411 state\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     5.375 brother\n",
      "     4.316 sister\n",
      "     3.003 Marlon\n",
      "     ..... .....\n",
      "    -1.211 starring\n",
      "    -1.229 –\n",
      "    -1.402 from\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     3.536 capital\n",
      "     1.747 city\n",
      "     1.727 posted\n",
      "     ..... .....\n",
      "    -1.196 and\n",
      "    -1.257 borough\n",
      "    -1.408 States\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     3.076 \n",
      "     2.535 vocalist\n",
      "     2.377 philosopher\n",
      "     ..... .....\n",
      "    -1.249 in\n",
      "    -1.256 Texas\n",
      "    -2.147 on\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     5.509 wife\n",
      "     4.420 married\n",
      "     4.367 widow\n",
      "     ..... .....\n",
      "    -1.282 Tyndareus\n",
      "    -1.319 children\n",
      "    -1.660 Terri\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different model factory [1 point]\n",
    "\n",
    "The code in `rel_ext` makes it very easy to experiment with other classifier models: one need only redefine the `model_factory` argument. This question asks you to assess a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "__To submit:__ A call to `rel_ext.experiment` training on the 'train' part of `splits` and assessing on its `dev` part, with `featurizers` as defined above in this notebook and the `model_factory` set to one based in an `SVC` with `kernel='linear'` and all other arguments left with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.788      0.318      0.608        340       5716\n",
      "parents                   0.780      0.590      0.732        312       5688\n",
      "founders                  0.790      0.445      0.684        380       5756\n",
      "genre                     0.558      0.253      0.450        170       5546\n",
      "worked_at                 0.627      0.306      0.518        242       5618\n",
      "contains                  0.780      0.608      0.739       3904       9280\n",
      "author                    0.777      0.601      0.734        509       5885\n",
      "nationality               0.571      0.199      0.416        301       5677\n",
      "film_performance          0.767      0.610      0.729        766       6142\n",
      "place_of_birth            0.578      0.223      0.438        233       5609\n",
      "is_a                      0.609      0.270      0.487        497       5873\n",
      "place_of_death            0.395      0.094      0.241        159       5535\n",
      "has_sibling               0.816      0.240      0.552        499       5875\n",
      "capital                   0.636      0.295      0.517         95       5471\n",
      "profession                0.629      0.267      0.495        247       5623\n",
      "has_spouse                0.851      0.347      0.659        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.685      0.354      0.562       9248      95264\n"
     ]
    }
   ],
   "source": [
    "svc_model_factory = lambda: SVC(kernel='linear')\n",
    "\n",
    "svc_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=svc_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional unigram features [2 points]\n",
    "\n",
    "The current bag-of-words representation makes no distinction between \"forward\" and \"reverse\" examples. But, intuitively, there is big difference between _X and his son Y_ and _Y and his son X_. This question asks you to modify `simple_bag_of_words_featurizer` to capture these differences. \n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `directional_bag_of_words_featurizer` that is just like `simple_bag_of_words_featurizer` except that it distinguishes \"forward\" and \"reverse\". To do this, you just need to mark each word feature for whether it is derived from a subject–object example or from an object–subject example. The precise nature of the mark you add for the two cases doesn't make a difference to the model.\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `directional_bag_of_words_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "3. `rel_ext.experiment` returns some of the core objects used in the experiment. How many feature names does the `vectorizer` have for the experiment run in the previous step? (Note: we're partly asking you to figure out how to get this value by using the sklearn documentation, so please don't ask how to do it on Piazza!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "worked_at                 0.733      0.273      0.548        242       5618\n",
      "profession                0.716      0.235      0.508        247       5623\n",
      "capital                   0.571      0.253      0.456         95       5471\n",
      "genre                     0.784      0.235      0.535        170       5546\n",
      "adjoins                   0.833      0.412      0.692        340       5716\n",
      "place_of_death            0.512      0.138      0.332        159       5535\n",
      "is_a                      0.717      0.249      0.521        497       5873\n",
      "parents                   0.844      0.519      0.750        312       5688\n",
      "has_spouse                0.847      0.354      0.662        594       5970\n",
      "film_performance          0.842      0.653      0.796        766       6142\n",
      "place_of_birth            0.684      0.232      0.492        233       5609\n",
      "founders                  0.851      0.389      0.688        380       5756\n",
      "has_sibling               0.805      0.248      0.556        499       5875\n",
      "author                    0.841      0.583      0.773        509       5885\n",
      "nationality               0.663      0.223      0.475        301       5677\n",
      "contains                  0.804      0.672      0.774       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.753      0.354      0.597       9248      95264\n"
     ]
    }
   ],
   "source": [
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter, fwd_prefix='$FWD_DIRECTION: ', \n",
    "                                        bwd_prefix='$BWD_DIREECTION: ', use_middle_length=False,\n",
    "                                        use_entities=False):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            word_direction = fwd_prefix + word\n",
    "            feature_counter[word_direction] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['FWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            word_direction = bwd_prefix + word\n",
    "            feature_counter[word_direction] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['FWD_NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "    return feature_counter\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The part-of-speech tags of the \"middle\" words [2 points]\n",
    "\n",
    "Our corpus distribution contains part-of-speech (POS) tagged versions of the core text spans. Let's begin to explore whether there is information in these sequences, focusing on `middle_POS`.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `middle_bigram_pos_tag_featurizer` that is just like `simple_bag_of_words_featurizer` except that it creates a feature for bigram POS sequences. For example, given \n",
    "\n",
    "  `The/DT dog/N napped/V`\n",
    "  \n",
    "   we obtain the list of bigram POS sequences\n",
    "  \n",
    "   `b = ['<s> DT', 'DT N', 'N V', 'V </s>']`. \n",
    "   \n",
    "   Of course, `middle_bigram_pos_tag_featurizer` should return count dictionaries defined in terms of such bigram POS lists, on the model of `simple_bag_of_words_featurizer`.\n",
    "   \n",
    "   Don't forget the start and end tags, to model those environments properly!\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `middle_bigram_pos_tag_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "Note: To parse `middle_POS`, one splits on whitespace to get the `word/TAG` pairs. Each of these pairs `s` can be parsed with `s.rsplit('/', 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.899      0.391      0.714        340       5716\n",
      "parents                   0.732      0.228      0.507        312       5688\n",
      "founders                  0.691      0.147      0.398        380       5756\n",
      "genre                     0.824      0.082      0.294        170       5546\n",
      "worked_at                 0.614      0.145      0.372        242       5618\n",
      "contains                  0.707      0.297      0.554       3904       9280\n",
      "author                    0.817      0.246      0.558        509       5885\n",
      "nationality               0.662      0.163      0.410        301       5677\n",
      "film_performance          0.749      0.242      0.527        766       6142\n",
      "place_of_birth            0.723      0.202      0.477        233       5609\n",
      "is_a                      0.707      0.165      0.427        497       5873\n",
      "place_of_death            0.615      0.101      0.304        159       5535\n",
      "has_sibling               0.714      0.170      0.436        499       5875\n",
      "capital                   0.692      0.095      0.306         95       5471\n",
      "profession                0.750      0.194      0.477        247       5623\n",
      "has_spouse                0.793      0.271      0.573        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.731      0.196      0.458       9248      95264\n"
     ]
    }
   ],
   "source": [
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        word_POSs = ex.middle_POS.split(' ')\n",
    "        len_POS = len(word_POSs)\n",
    "        for i in range(-1, len_POS - 1):\n",
    "            pos = word_POSs[i].rsplit('/', 1)\n",
    "            bigram = \"\"\n",
    "            if len(pos) > 1:\n",
    "                if i == -1:\n",
    "                    bigram = '<s> ' + pos[1]\n",
    "                elif i == len_POS - 2:\n",
    "                    bigram = pos[1] + ' </s>'\n",
    "                else:\n",
    "                    bigram = pos[1] + \" \" + word_POSs[i+1].rsplit('/', 1)[1]\n",
    "            feature_counter[bigram] += 1\n",
    "    return feature_counter\n",
    "\n",
    "ngram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[middle_bigram_pos_tag_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "There are many options, and this could easily grow into a project. Here are a few ideas:\n",
    "\n",
    "- Try out different classifier models, from `sklearn` and elsewhere.\n",
    "- Add a feature that indicates the length of the middle.\n",
    "- Augment the bag-of-words representation to include bigrams or trigrams (not just unigrams).\n",
    "- Introduce features based on the entity mentions themselves. <!-- \\[SPOILER: it helps a lot, maybe 4% in F-score. And combines nicely with the directional features.\\] -->\n",
    "- Experiment with features based on the context outside (rather than between) the two entity mentions — that is, the words before the first mention, or after the second.\n",
    "- Try adding features which capture syntactic information, such as the dependency-path features used by Mintz et al. 2009. The [NLTK](https://www.nltk.org/) toolkit contains a variety of [parsing algorithms](http://www.nltk.org/api/nltk.parse.html) that may help.\n",
    "- The bag-of-words representation does not permit generalization across word categories such as names of people, places, or companies. Can we do better using word embeddings such as [GloVe](https://nlp.stanford.edu/projects/glove/)?\n",
    "- Consider adding features based on WordNet synsets. Here's a little code to get you started with that:\n",
    "  ```\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  dog_compatible_synsets = wn.synsets('dog', pos='n')\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_compatible_synsets = wn.synsets('dog', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-4fb974f47ea6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-4fb974f47ea6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dog_compatible_synsets[0].??\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dog_compatible_synsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.924      0.250      0.600        340       5716\n",
      "parents                   0.000      0.000      0.000        312       5688\n",
      "founders                  0.000      0.000      0.000        380       5756\n",
      "genre                     0.000      0.000      0.000        170       5546\n",
      "worked_at                 0.000      0.000      0.000        242       5618\n",
      "contains                  0.891      0.140      0.431       3904       9280\n",
      "author                    0.833      0.020      0.090        509       5885\n",
      "nationality               0.000      0.000      0.000        301       5677\n",
      "film_performance          0.000      0.000      0.000        766       6142\n",
      "place_of_birth            0.000      0.000      0.000        233       5609\n",
      "is_a                      0.000      0.000      0.000        497       5873\n",
      "place_of_death            0.000      0.000      0.000        159       5535\n",
      "has_sibling               0.000      0.000      0.000        499       5875\n",
      "capital                   1.000      0.011      0.051         95       5471\n",
      "profession                0.000      0.000      0.000        247       5623\n",
      "has_spouse                1.000      0.013      0.064        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.291      0.027      0.077       9248      95264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc_rbf_model_factory = lambda: SVC(kernel='rbf')\n",
    "\n",
    "svc_rbf_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=svc_rbf_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   1.000      0.091      0.334        340       5716\n",
      "parents                   0.000      0.000      0.000        312       5688\n",
      "founders                  0.000      0.000      0.000        380       5756\n",
      "genre                     0.000      0.000      0.000        170       5546\n",
      "worked_at                 0.000      0.000      0.000        242       5618\n",
      "contains                  1.000      0.001      0.003       3904       9280\n",
      "author                    0.000      0.000      0.000        509       5885\n",
      "nationality               0.000      0.000      0.000        301       5677\n",
      "film_performance          0.000      0.000      0.000        766       6142\n",
      "place_of_birth            0.000      0.000      0.000        233       5609\n",
      "is_a                      0.000      0.000      0.000        497       5873\n",
      "place_of_death            0.000      0.000      0.000        159       5535\n",
      "has_sibling               0.000      0.000      0.000        499       5875\n",
      "capital                   0.000      0.000      0.000         95       5471\n",
      "profession                0.000      0.000      0.000        247       5623\n",
      "has_spouse                0.000      0.000      0.000        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.125      0.006      0.021       9248      95264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc_poly_model_factory = lambda: SVC(kernel='poly')\n",
    "\n",
    "svc_poly_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=svc_poly_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.462      0.524      0.473        340       5716\n",
      "parents                   0.910      0.356      0.694        312       5688\n",
      "founders                  0.866      0.271      0.602        380       5756\n",
      "genre                     0.800      0.024      0.105        170       5546\n",
      "worked_at                 0.955      0.087      0.318        242       5618\n",
      "contains                  0.723      0.710      0.721       3904       9280\n",
      "author                    0.779      0.485      0.695        509       5885\n",
      "nationality               0.815      0.073      0.269        301       5677\n",
      "film_performance          0.817      0.529      0.736        766       6142\n",
      "place_of_birth            0.500      0.004      0.021        233       5609\n",
      "is_a                      0.743      0.105      0.335        497       5873\n",
      "place_of_death            0.000      0.000      0.000        159       5535\n",
      "has_sibling               0.848      0.156      0.450        499       5875\n",
      "capital                   0.450      0.095      0.257         95       5471\n",
      "profession                0.500      0.008      0.038        247       5623\n",
      "has_spouse                0.809      0.300      0.604        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.686      0.233      0.395       9248      95264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "gaussian_nb_model_factory = lambda: MultinomialNB()\n",
    "\n",
    "gaussian_nb_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=gaussian_nb_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "simple_bag_of_words_middle_featurizer = partial(simple_bag_of_words_featurizer,use_middle_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.870      0.353      0.673        340       5716\n",
      "parents                   0.838      0.532      0.752        312       5688\n",
      "founders                  0.856      0.408      0.702        380       5756\n",
      "genre                     0.580      0.171      0.392        170       5546\n",
      "worked_at                 0.765      0.256      0.548        242       5618\n",
      "contains                  0.801      0.595      0.749       3904       9280\n",
      "author                    0.846      0.507      0.746        509       5885\n",
      "nationality               0.679      0.176      0.432        301       5677\n",
      "film_performance          0.790      0.546      0.725        766       6142\n",
      "place_of_birth            0.667      0.197      0.452        233       5609\n",
      "is_a                      0.681      0.219      0.479        497       5873\n",
      "place_of_death            0.513      0.126      0.317        159       5535\n",
      "has_sibling               0.870      0.240      0.571        499       5875\n",
      "capital                   0.543      0.200      0.404         95       5471\n",
      "profession                0.667      0.178      0.431        247       5623\n",
      "has_spouse                0.863      0.328      0.651        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.739      0.315      0.564       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bow_middle_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_middle_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_bag_of_words_featurizer(kbt, corpus, feature_counter, n=2, use_middle_length=False):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for i in range(0, len(words), n):\n",
    "            end = i + n\n",
    "            if (len(words) - i) < n:\n",
    "                end = len(words)\n",
    "            n_gram = ' '.join(words[i:end])\n",
    "            feature_counter[n_gram] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for i in range(0, len(words), n):\n",
    "            end = i + n\n",
    "            if (len(words) - i) < n:\n",
    "                end = len(words)\n",
    "            n_gram = ' '.join(words[i:end])\n",
    "            feature_counter[n_gram] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_bag_of_words_featurizer = partial(ngrams_bag_of_words_featurizer, n=2)\n",
    "trigrams_bag_of_words_featurizer = partial(ngrams_bag_of_words_featurizer, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.906      0.341      0.681        340       5716\n",
      "parents                   0.871      0.391      0.700        312       5688\n",
      "founders                  0.809      0.279      0.586        380       5756\n",
      "genre                     0.725      0.171      0.439        170       5546\n",
      "worked_at                 0.793      0.190      0.485        242       5618\n",
      "contains                  0.807      0.567      0.744       3904       9280\n",
      "author                    0.810      0.470      0.708        509       5885\n",
      "nationality               0.625      0.133      0.359        301       5677\n",
      "film_performance          0.853      0.456      0.726        766       6142\n",
      "place_of_birth            0.683      0.185      0.443        233       5609\n",
      "is_a                      0.773      0.199      0.491        497       5873\n",
      "place_of_death            0.500      0.069      0.223        159       5535\n",
      "has_sibling               0.871      0.202      0.524        499       5875\n",
      "capital                   0.615      0.168      0.402         95       5471\n",
      "profession                0.826      0.154      0.441        247       5623\n",
      "has_spouse                0.876      0.274      0.609        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.772      0.266      0.535       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[bigrams_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.936      0.303      0.660        340       5716\n",
      "parents                   0.879      0.279      0.614        312       5688\n",
      "founders                  0.792      0.200      0.497        380       5756\n",
      "genre                     0.839      0.153      0.442        170       5546\n",
      "worked_at                 0.805      0.136      0.406        242       5618\n",
      "contains                  0.793      0.481      0.702       3904       9280\n",
      "author                    0.814      0.438      0.695        509       5885\n",
      "nationality               0.707      0.136      0.385        301       5677\n",
      "film_performance          0.857      0.376      0.682        766       6142\n",
      "place_of_birth            0.733      0.094      0.312        233       5609\n",
      "is_a                      0.674      0.117      0.345        497       5873\n",
      "place_of_death            0.583      0.044      0.169        159       5535\n",
      "has_sibling               0.829      0.136      0.411        499       5875\n",
      "capital                   0.522      0.126      0.321         95       5471\n",
      "profession                0.629      0.089      0.284        247       5623\n",
      "has_spouse                0.907      0.229      0.570        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.769      0.209      0.468       9248      95264\n"
     ]
    }
   ],
   "source": [
    "trigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[trigrams_bag_of_words_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.916      0.321      0.668        340       5716\n",
      "parents                   0.865      0.391      0.696        312       5688\n",
      "founders                  0.835      0.279      0.597        380       5756\n",
      "genre                     0.794      0.159      0.441        170       5546\n",
      "worked_at                 0.797      0.194      0.492        242       5618\n",
      "contains                  0.814      0.549      0.742       3904       9280\n",
      "author                    0.835      0.458      0.717        509       5885\n",
      "nationality               0.707      0.136      0.385        301       5677\n",
      "film_performance          0.838      0.461      0.720        766       6142\n",
      "place_of_birth            0.731      0.163      0.431        233       5609\n",
      "is_a                      0.746      0.189      0.470        497       5873\n",
      "place_of_death            0.556      0.063      0.216        159       5535\n",
      "has_sibling               0.874      0.180      0.494        499       5875\n",
      "capital                   0.619      0.137      0.363         95       5471\n",
      "profession                0.733      0.178      0.452        247       5623\n",
      "has_spouse                0.884      0.256      0.593        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.784      0.257      0.530       9248      95264\n"
     ]
    }
   ],
   "source": [
    "bigrams_bag_of_words_featurizer_use_middle = partial(ngrams_bag_of_words_featurizer, n=2, use_middle_length=True)\n",
    "\n",
    "bigram_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[bigrams_bag_of_words_featurizer_use_middle],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.919      0.432      0.750        340       5716\n",
      "parents                   0.860      0.532      0.766        312       5688\n",
      "founders                  0.867      0.379      0.690        380       5756\n",
      "genre                     0.884      0.359      0.684        170       5546\n",
      "worked_at                 0.765      0.269      0.558        242       5618\n",
      "contains                  0.777      0.341      0.619       3904       9280\n",
      "author                    0.831      0.521      0.742        509       5885\n",
      "nationality               0.623      0.269      0.493        301       5677\n",
      "film_performance          0.822      0.531      0.741        766       6142\n",
      "place_of_birth            0.667      0.215      0.469        233       5609\n",
      "is_a                      0.816      0.392      0.671        497       5873\n",
      "place_of_death            0.487      0.119      0.302        159       5535\n",
      "has_sibling               0.939      0.246      0.601        499       5875\n",
      "capital                   0.586      0.179      0.403         95       5471\n",
      "profession                0.852      0.397      0.693        247       5623\n",
      "has_spouse                0.889      0.296      0.635        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.787      0.342      0.614       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_entities_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True)\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_entities_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.890      0.379      0.701        340       5716\n",
      "parents                   0.533      0.103      0.290        312       5688\n",
      "founders                  0.604      0.084      0.270        380       5756\n",
      "genre                     0.795      0.206      0.506        170       5546\n",
      "worked_at                 0.605      0.107      0.314        242       5618\n",
      "contains                  0.698      0.413      0.613       3904       9280\n",
      "author                    0.764      0.318      0.597        509       5885\n",
      "nationality               0.513      0.130      0.322        301       5677\n",
      "film_performance          0.741      0.462      0.661        766       6142\n",
      "place_of_birth            0.365      0.082      0.215        233       5609\n",
      "is_a                      0.659      0.276      0.515        497       5873\n",
      "place_of_death            0.238      0.031      0.103        159       5535\n",
      "has_sibling               0.723      0.240      0.516        499       5875\n",
      "capital                   0.469      0.158      0.336         95       5471\n",
      "profession                0.699      0.206      0.473        247       5623\n",
      "has_spouse                0.614      0.131      0.354        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.619      0.208      0.424       9248      95264\n"
     ]
    }
   ],
   "source": [
    "left_bag_of_words_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True, context_section='left')\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[left_bag_of_words_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.886      0.388      0.705        340       5716\n",
      "parents                   0.478      0.106      0.281        312       5688\n",
      "founders                  0.473      0.068      0.217        380       5756\n",
      "genre                     0.860      0.218      0.541        170       5546\n",
      "worked_at                 0.500      0.079      0.241        242       5618\n",
      "contains                  0.645      0.385      0.569       3904       9280\n",
      "author                    0.672      0.242      0.496        509       5885\n",
      "nationality               0.424      0.120      0.281        301       5677\n",
      "film_performance          0.689      0.393      0.599        766       6142\n",
      "place_of_birth            0.280      0.060      0.162        233       5609\n",
      "is_a                      0.647      0.243      0.486        497       5873\n",
      "place_of_death            0.368      0.044      0.149        159       5535\n",
      "has_sibling               0.657      0.142      0.381        499       5875\n",
      "capital                   0.472      0.179      0.356         95       5471\n",
      "profession                0.643      0.182      0.427        247       5623\n",
      "has_spouse                0.545      0.103      0.293        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.577      0.184      0.386       9248      95264\n"
     ]
    }
   ],
   "source": [
    "right_bag_of_words_featurizer = partial(simple_bag_of_words_featurizer, use_entities=True, context_section='right')\n",
    "\n",
    "use_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[right_bag_of_words_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.838      0.503      0.740        340       5716\n",
      "parents                   0.800      0.538      0.729        312       5688\n",
      "founders                  0.818      0.426      0.691        380       5756\n",
      "genre                     0.781      0.588      0.733        170       5546\n",
      "worked_at                 0.653      0.335      0.549        242       5618\n",
      "contains                  0.774      0.403      0.654       3904       9280\n",
      "author                    0.787      0.574      0.733        509       5885\n",
      "nationality               0.510      0.326      0.458        301       5677\n",
      "film_performance          0.776      0.610      0.736        766       6142\n",
      "place_of_birth            0.496      0.262      0.421        233       5609\n",
      "is_a                      0.765      0.551      0.710        497       5873\n",
      "place_of_death            0.318      0.170      0.271        159       5535\n",
      "has_sibling               0.863      0.317      0.642        499       5875\n",
      "capital                   0.550      0.232      0.431         95       5471\n",
      "profession                0.788      0.543      0.723        247       5623\n",
      "has_spouse                0.825      0.325      0.631        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.709      0.419      0.616       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_middle_entities_featurizer = partial(simple_bag_of_words_featurizer, \n",
    "                                                         use_entities=True, use_middle_length=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_middle_entities_featurizer],\n",
    "    model_factory=svc_model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer2(kbt, corpus, feature_counter, \n",
    "                                    use_middle_length=False, \n",
    "                                    use_entities=False,\n",
    "                                    context_section='middle', # can be 'left', 'right', or 'middle'\n",
    "                                    use_synsets=False):\n",
    "    synset_prefix = \"synset_:\"\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "        \n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos_pair in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[word] += 1\n",
    "                        for hyponym in syn.hyponyms():\n",
    "                            feature_counter[synset_prefix+hyponym.name()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        \n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        else:\n",
    "            words = ex.middle.split(' ')\n",
    "\n",
    "        if use_synsets:            \n",
    "            pos_s = ex.middle_POS.split(' ')\n",
    "            for word, pos_pair in zip(words,pos_s):\n",
    "                if word not in string.punctuation:\n",
    "                    feature_counter[word] += 1\n",
    "                    pos_split = pos_pair.rsplit('/', 1)\n",
    "                    word, pos_word = pos_split[0], pos_split[1]\n",
    "                    synsets = wn.synsets(word)\n",
    "                    for syn in synsets:\n",
    "                        feature_counter[word] += 1\n",
    "                        for hyponym in syn.hyponyms():\n",
    "                            feature_counter[synset_prefix+hyponym.name()] += 1\n",
    "        else: \n",
    "            for word in words:\n",
    "                feature_counter[word] += 1\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "        if use_entities:\n",
    "            feature_counter[kbt.sbj] += 1\n",
    "            feature_counter[kbt.obj] += 1\n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "worked_at                 0.526      0.293      0.454        242       5618\n",
      "profession                0.442      0.215      0.365        247       5623\n",
      "capital                   0.397      0.284      0.368         95       5471\n",
      "genre                     0.468      0.300      0.421        170       5546\n",
      "adjoins                   0.684      0.344      0.571        340       5716\n",
      "place_of_death            0.267      0.126      0.218        159       5535\n",
      "is_a                      0.494      0.264      0.421        497       5873\n",
      "parents                   0.752      0.554      0.702        312       5688\n",
      "has_spouse                0.722      0.328      0.582        594       5970\n",
      "film_performance          0.722      0.624      0.700        766       6142\n",
      "place_of_birth            0.441      0.223      0.369        233       5609\n",
      "founders                  0.679      0.445      0.614        380       5756\n",
      "has_sibling               0.632      0.255      0.487        499       5875\n",
      "author                    0.728      0.611      0.701        509       5885\n",
      "nationality               0.355      0.216      0.315        301       5677\n",
      "contains                  0.756      0.594      0.716       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.567      0.355      0.500       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_synsets_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                                         use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_synsets_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "worked_at                 0.742      0.298      0.571        242       5618\n",
      "profession                0.734      0.368      0.612        247       5623\n",
      "capital                   0.542      0.274      0.453         95       5471\n",
      "genre                     0.800      0.400      0.667        170       5546\n",
      "adjoins                   0.869      0.450      0.733        340       5716\n",
      "place_of_death            0.375      0.132      0.274        159       5535\n",
      "is_a                      0.786      0.384      0.650        497       5873\n",
      "parents                   0.796      0.564      0.736        312       5688\n",
      "has_spouse                0.864      0.320      0.645        594       5970\n",
      "film_performance          0.807      0.584      0.749        766       6142\n",
      "place_of_birth            0.583      0.240      0.454        233       5609\n",
      "founders                  0.756      0.400      0.642        380       5756\n",
      "has_sibling               0.818      0.253      0.565        499       5875\n",
      "author                    0.770      0.591      0.726        509       5885\n",
      "nationality               0.547      0.292      0.466        301       5677\n",
      "contains                  0.785      0.392      0.654       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.723      0.371      0.600       9248      95264\n"
     ]
    }
   ],
   "source": [
    "simple_bag_of_words_all_featurizer = partial(simple_bag_of_words_featurizer2, \n",
    "                                            use_entities=True, use_middle_length=True, use_synsets=True)\n",
    "\n",
    "use_middle_entities_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[simple_bag_of_words_all_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.873      0.385      0.697        340       5716\n",
      "parents                   0.846      0.510      0.747        312       5688\n",
      "founders                  0.856      0.421      0.709        380       5756\n",
      "genre                     0.710      0.259      0.526        170       5546\n",
      "worked_at                 0.821      0.264      0.578        242       5618\n",
      "contains                  0.846      0.665      0.803       3904       9280\n",
      "author                    0.868      0.580      0.789        509       5885\n",
      "nationality               0.649      0.203      0.451        301       5677\n",
      "film_performance          0.863      0.648      0.809        766       6142\n",
      "place_of_birth            0.736      0.227      0.509        233       5609\n",
      "is_a                      0.773      0.239      0.535        497       5873\n",
      "place_of_death            0.605      0.145      0.370        159       5535\n",
      "has_sibling               0.882      0.240      0.575        499       5875\n",
      "capital                   0.579      0.232      0.445         95       5471\n",
      "profession                0.853      0.235      0.559        247       5623\n",
      "has_spouse                0.867      0.352      0.671        594       5970\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.789      0.350      0.611       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_middle_featurizer = partial(directional_bag_of_words_featurizer, use_middle_length=True)\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_middle_featurizer],\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "worked_at                 0.762      0.264      0.554        242       5618\n",
      "profession                0.867      0.421      0.715        247       5623\n",
      "capital                   0.632      0.253      0.486         95       5471\n",
      "genre                     0.883      0.400      0.711        170       5546\n",
      "adjoins                   0.902      0.432      0.741        340       5716\n",
      "place_of_death            0.583      0.176      0.399        159       5535\n",
      "is_a                      0.842      0.396      0.687        497       5873\n",
      "parents                   0.859      0.510      0.756        312       5688\n",
      "has_spouse                0.894      0.313      0.652        594       5970\n",
      "film_performance          0.854      0.634      0.799        766       6142\n",
      "place_of_birth            0.659      0.249      0.496        233       5609\n",
      "founders                  0.848      0.368      0.673        380       5756\n",
      "has_sibling               0.893      0.251      0.590        499       5875\n",
      "author                    0.858      0.631      0.800        509       5885\n",
      "nationality               0.690      0.326      0.564        301       5677\n",
      "contains                  0.841      0.396      0.687       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.804      0.376      0.644       9248      95264\n"
     ]
    }
   ],
   "source": [
    "directional_middle_entities_featurizer = partial(directional_bag_of_words_featurizer, \n",
    "                                                 use_middle_length=True, use_entities=True)\n",
    "\n",
    "directional_bag_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[directional_middle_entities_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_bag_of_words_featurizer(kbt, corpus, feature_counter, glove_lookup,\n",
    "                                context_section='middle',\n",
    "                                use_middle_length=False,\n",
    "                                glove_dims=300): # can be 'left', 'right', or 'middle'\n",
    "    glove_vector = np.zeros(glove_dims)\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            #words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "            words = ' '.join((ex.mention_1, ex.middle, ex.mention_2)).split(' ')\n",
    "            for word in words:\n",
    "                glove_vector += glove_lookup.get(word, np.zeros(glove_dims))\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "            \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = None\n",
    "        if context_section == 'left':\n",
    "            words = ex.left.split(' ')\n",
    "        elif context_section == 'right':\n",
    "            words = ex.right.split(' ')\n",
    "        elif context_section == 'middle':\n",
    "            words = ex.middle.split(' ')\n",
    "        else:\n",
    "            #words = ' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right)).split(' ')\n",
    "            words = ' '.join((ex.mention_1, ex.middle, ex.mention_2)).split(' ')\n",
    "            for word in words:\n",
    "                glove_vector += glove_lookup.get(word, np.zeros(glove_dims))\n",
    "        if use_middle_length:\n",
    "            feature_counter['NUM_WORD_IN_MIDDLE']  += len(words)\n",
    "    \n",
    "    feature_prefix = \"glove_:\"\n",
    "    for i, feature in enumerate(glove_vector):\n",
    "        feature_counter[feature_prefix + str(i)] = feature\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_lookup['sweden'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "worked_at                 0.619      0.248      0.476        242       5618\n",
      "profession                0.624      0.377      0.552        247       5623\n",
      "capital                   0.481      0.274      0.418         95       5471\n",
      "genre                     0.647      0.453      0.596        170       5546\n",
      "adjoins                   0.850      0.450      0.722        340       5716\n",
      "place_of_death            0.317      0.119      0.238        159       5535\n",
      "is_a                      0.699      0.346      0.581        497       5873\n",
      "parents                   0.740      0.519      0.682        312       5688\n",
      "has_spouse                0.748      0.340      0.603        594       5970\n",
      "film_performance          0.767      0.554      0.712        766       6142\n",
      "place_of_birth            0.584      0.223      0.441        233       5609\n",
      "founders                  0.718      0.403      0.621        380       5756\n",
      "has_sibling               0.727      0.257      0.532        499       5875\n",
      "author                    0.760      0.436      0.662        509       5885\n",
      "nationality               0.533      0.159      0.363        301       5677\n",
      "contains                  0.810      0.594      0.755       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.664      0.359      0.560       9248      95264\n"
     ]
    }
   ],
   "source": [
    "glove_featurizer = partial(glove_bag_of_words_featurizer, context_section='all', glove_lookup=glove_lookup)\n",
    "\n",
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[glove_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "worked_at                 0.604      0.252      0.472        242       5618\n",
      "profession                0.628      0.397      0.563        247       5623\n",
      "capital                   0.466      0.284      0.413         95       5471\n",
      "genre                     0.610      0.441      0.566        170       5546\n",
      "adjoins                   0.851      0.453      0.724        340       5716\n",
      "place_of_death            0.317      0.119      0.238        159       5535\n",
      "is_a                      0.654      0.350      0.557        497       5873\n",
      "parents                   0.750      0.529      0.692        312       5688\n",
      "has_spouse                0.750      0.359      0.616        594       5970\n",
      "film_performance          0.773      0.559      0.718        766       6142\n",
      "place_of_birth            0.622      0.219      0.455        233       5609\n",
      "founders                  0.750      0.426      0.651        380       5756\n",
      "has_sibling               0.731      0.257      0.534        499       5875\n",
      "author                    0.767      0.440      0.668        509       5885\n",
      "nationality               0.533      0.159      0.363        301       5677\n",
      "contains                  0.805      0.593      0.752       3904       9280\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.663      0.365      0.561       9248      95264\n"
     ]
    }
   ],
   "source": [
    "glove_length_featurizer = partial(glove_bag_of_words_featurizer, \n",
    "                                  context_section='all', \n",
    "                                  use_middle_length=True, glove_lookup=glove_lookup)\n",
    "\n",
    "glove_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=[glove_length_featurizer],\n",
    "    model_factory=model_factory_400,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release a test set right after class on April 29. The announcement will go out on Piazza. You will evaluate your custom model from the previous question on these new datasets using the function `rel_ext.bake_off_experiment`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187248\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on May 1. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average f-score (an F_0.5 score) as reported \n",
    "# by the code above. Please enter only a number between \n",
    "# 0 and 1 inclusive. Please do not remove this comment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
